# ğŸš€ ì„±ëŠ¥ ê°œì„  ì „ëµ: From 49.7% to 70%+

## í˜„ì¬ ìƒí™©

```
í˜„ì¬ ì„±ëŠ¥:
- Within 5%p: 49.7% âŒ
- Within 10%p: 85.7% âš ï¸
- MAE: 0.0715

ëª©í‘œ:
- Within 5%p: 70%+ ğŸ¯
- Within 10%p: 95%+
- MAE: 0.050~0.060

ê°œì„  í•„ìš”: +20.3%p
```

---

## ğŸ“‹ ëª©ì°¨

1. [ì¦‰ì‹œ ê°œì„  ê°€ëŠ¥ (ë‹¨ê¸°)](#1-ì¦‰ì‹œ-ê°œì„ -ê°€ëŠ¥-ë‹¨ê¸°)
2. [ë°ì´í„° ì¶”ê°€ (ì¤‘ê¸°)](#2-ë°ì´í„°-ì¶”ê°€-ì¤‘ê¸°)
3. [ëª¨ë¸ ê³ ë„í™” (ì¤‘ê¸°)](#3-ëª¨ë¸-ê³ ë„í™”-ì¤‘ê¸°)
4. [ì•™ìƒë¸” ì „ëµ (ì¤‘ê¸°)](#4-ì•™ìƒë¸”-ì „ëµ-ì¤‘ê¸°)
5. [ì·¨ì•½ êµ¬ê°„ ì§‘ì¤‘ (ì¤‘ê¸°)](#5-ì·¨ì•½-êµ¬ê°„-ì§‘ì¤‘-ì¤‘ê¸°)
6. [ì¥ê¸° ì „ëµ](#6-ì¥ê¸°-ì „ëµ)

---

## 1. ì¦‰ì‹œ ê°œì„  ê°€ëŠ¥ (ë‹¨ê¸°)

### 1.1 Huber í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

**í˜„ì¬:**
```python
HuberRegressor(epsilon=1.35, alpha=0.0001, max_iter=100)
```

**ê°œì„  ì „ëµ:**

```python
from sklearn.model_selection import GridSearchCV

# íƒìƒ‰ ê³µê°„
param_grid = {
    'epsilon': [1.1, 1.2, 1.35, 1.5, 2.0],
    'alpha': [0.00001, 0.0001, 0.001, 0.01],
    'max_iter': [100, 200, 500]
}

# Grid Search
grid = GridSearchCV(
    HuberRegressor(),
    param_grid,
    cv=5,
    scoring='neg_mean_absolute_error',
    n_jobs=-1
)

grid.fit(X_train_scaled, y_train)

print(f"Best params: {grid.best_params_}")
print(f"Best MAE: {-grid.best_score_:.4f}")
```

**ê¸°ëŒ€ íš¨ê³¼:** MAE 0.0715 â†’ 0.0700 (~2% ê°œì„ )

---

### 1.2 ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ë¯¸ì„¸ ì¡°ì •

**í˜„ì¬:**
```python
Ensemble = Huber 0.8 + CatBoost 0.2
MAE: 0.0715
```

**ê°œì„  ì „ëµ:**

```python
# ë” ì„¸ë°€í•œ íƒìƒ‰ (0.01 ë‹¨ìœ„)
weights = np.arange(0.70, 0.90, 0.01)

best_mae = float('inf')
best_weight = 0

for w_h in weights:
    w_c = 1 - w_h
    
    pred = w_h * huber_pred + w_c * catboost_pred
    mae = mean_absolute_error(y_test, pred)
    
    if mae < best_mae:
        best_mae = mae
        best_weight = w_h

print(f"Optimal weight: Huber {best_weight:.2f}")
print(f"MAE: {best_mae:.4f}")
```

**ê¸°ëŒ€ íš¨ê³¼:** 0.0002~0.0005 ê°œì„  (ë¯¸ë¯¸)

---

### 1.3 ì´ìƒì¹˜ ì œê±° í›„ ì¬í•™ìŠµ

**ë¬¸ì œ:**
```
5%ê°€ Â±20%p ì´ìƒ ì˜¤ì°¨
â†’ í•™ìŠµ ë°©í•´
â†’ MAE ìƒìŠ¹
```

**ê°œì„  ì „ëµ:**

```python
# 1. ì´ìƒì¹˜ íƒì§€
from sklearn.ensemble import IsolationForest

iso = IsolationForest(contamination=0.05, random_state=42)
outliers = iso.fit_predict(X_train_scaled)

# ì •ìƒ ë°ì´í„°ë§Œ
X_train_clean = X_train_scaled[outliers == 1]
y_train_clean = y_train_array[outliers == 1]

print(f"ì œê±°: {(outliers == -1).sum()}ê°œ")
print(f"ë‚¨ìŒ: {(outliers == 1).sum()}ê°œ")

# 2. ì¬í•™ìŠµ
model = HuberRegressor(epsilon=1.35, alpha=0.0001)
model.fit(X_train_clean, y_train_clean)

# 3. í‰ê°€
pred = model.predict(X_test_scaled)
mae = mean_absolute_error(y_test, pred)
print(f"MAE (ì´ìƒì¹˜ ì œê±° í›„): {mae:.4f}")
```

**ê¸°ëŒ€ íš¨ê³¼:** MAE 0.0715 â†’ 0.0680 (~5% ê°œì„ )

---

### 1.4 íƒ€ê²Ÿ ë³€í™˜ (Log Transform)

**ë¬¸ì œ:**
```
íƒ€ê²Ÿ(ë‚™ì°°ê°€ìœ¨)ì´ ì™œê³¡ë¨
â†’ 0~2.0 ë²”ìœ„
â†’ ì´ìƒì¹˜ ë§ìŒ
```

**ê°œì„  ì „ëµ:**

```python
import numpy as np

# 1. Log ë³€í™˜
y_train_log = np.log1p(y_train_array)  # log(1+x)
y_test_log = np.log1p(y_test_array)

# 2. í•™ìŠµ
model = HuberRegressor(epsilon=1.35, alpha=0.0001)
model.fit(X_train_scaled, y_train_log)

# 3. ì˜ˆì¸¡ (ì—­ë³€í™˜)
pred_log = model.predict(X_test_scaled)
pred = np.expm1(pred_log)  # exp(x) - 1

# 4. í‰ê°€
mae = mean_absolute_error(y_test_array, pred)
print(f"MAE (Log Transform): {mae:.4f}")
```

**ê¸°ëŒ€ íš¨ê³¼:** MAE 0.0715 â†’ 0.0650 (~9% ê°œì„ )

---

## 2. ë°ì´í„° ì¶”ê°€ (ì¤‘ê¸°)

### 2.1 ì´ë¯¸ì§€ ë°ì´í„° (í¬ë¡¤ë§ ì¤‘)

**í˜„ì¬ ì§„í–‰ ì¤‘:**
- ë¬¼ê±´ ì‚¬ì§„ í¬ë¡¤ë§
- ì™¸ê´€/ì¸í…Œë¦¬ì–´ ì´ë¯¸ì§€

**êµ¬í˜„ ì „ëµ:**

```python
# 1. CNN íŠ¹ì§• ì¶”ì¶œ
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.preprocessing import image

# ì‚¬ì „í•™ìŠµ ëª¨ë¸
base_model = ResNet50(weights='imagenet', include_top=False, 
                      pooling='avg')

# ì´ë¯¸ì§€ â†’ íŠ¹ì§•
def extract_features(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x = preprocess_input(x)
    
    features = base_model.predict(x)
    return features.flatten()

# ëª¨ë“  ì´ë¯¸ì§€ ì²˜ë¦¬
image_features = []
for idx in df.index:
    img_path = f"images/{idx}.jpg"
    if os.path.exists(img_path):
        features = extract_features(img_path)
        image_features.append(features)
    else:
        image_features.append(np.zeros(2048))

image_features = np.array(image_features)

# 2. ê¸°ì¡´ í”¼ì²˜ì™€ ê²°í•©
X_combined = np.concatenate([X_scaled, image_features], axis=1)

# 3. í•™ìŠµ
model = HuberRegressor(epsilon=1.35, alpha=0.0001)
model.fit(X_combined_train, y_train)

# 4. í‰ê°€
pred = model.predict(X_combined_test)
mae = mean_absolute_error(y_test, pred)
print(f"MAE (with images): {mae:.4f}")
```

**ê¸°ëŒ€ íš¨ê³¼:** MAE 0.0715 â†’ 0.0500~0.0600 (15~30% ê°œì„ )

**ì´ìœ :**
- ì™¸ê´€ ìƒíƒœ (í˜ì¸íŠ¸, ê· ì—´)
- ì¸í…Œë¦¬ì–´ í’ˆì§ˆ (ë§ˆê°ì¬, êµ¬ì¡°)
- ì™„ì „íˆ ë…ë¦½ì  ì •ë³´!

---

### 2.2 ë“±ê¸°ë¶€ ì •ë³´

**ì¶”ê°€ ê°€ëŠ¥ ë°ì´í„°:**
```
ë²•ì  ì •ë³´:
- ê·¼ì €ë‹¹ ê°œìˆ˜
- ê°€ì••ë¥˜ ì—¬ë¶€
- ì „ì„¸ê¶Œ ì„¤ì •
- ì†Œìœ ê¶Œ ì´ì „ íšŸìˆ˜
- ê²½ë§¤ ì´ë ¥

ë¦¬ìŠ¤í¬ ì ìˆ˜:
- ë²•ì  ë³µì¡ë„ (0~10)
- ê¶Œë¦¬ ê´€ê³„ ë³µì¡ë„
```

**êµ¬í˜„:**

```python
# 1. í¬ë¡¤ë§ (ë²•ì› ë“±ê¸°ë¶€ë“±ë³¸)
def crawl_registry(case_id):
    # ...
    return {
        'ê·¼ì €ë‹¹ê°œìˆ˜': 2,
        'ê°€ì••ë¥˜': 1,
        'ì „ì„¸ê¶Œ': 0,
        'ì†Œìœ ê¶Œì´ì „': 3,
        'ë²•ì ë³µì¡ë„': 6
    }

# 2. í”¼ì²˜ ì¶”ê°€
registry_features = []
for case_id in df['case_id']:
    info = crawl_registry(case_id)
    registry_features.append([
        info['ê·¼ì €ë‹¹ê°œìˆ˜'],
        info['ê°€ì••ë¥˜'],
        info['ì „ì„¸ê¶Œ'],
        info['ì†Œìœ ê¶Œì´ì „'],
        info['ë²•ì ë³µì¡ë„']
    ])

X_with_registry = np.concatenate([X_scaled, registry_features], axis=1)

# 3. í•™ìŠµ ë° í‰ê°€
# ...
```

**ê¸°ëŒ€ íš¨ê³¼:** MAE 0.0715 â†’ 0.0600~0.0650 (10~15% ê°œì„ )

---

### 2.3 ì„ì°¨ì¸ í˜„í™©

**ì¶”ê°€ ì •ë³´:**
```
ì„ì°¨ì¸ ì •ë³´:
- ì„ì°¨ì¸ ìˆ˜
- ì´ ë³´ì¦ê¸ˆ
- ëŒ€í•­ë ¥ ìˆëŠ” ì„ì°¨ì¸ ìˆ˜
- ì†Œì•¡ì„ì°¨ì¸ ìˆ˜
- ëª…ë„ ë‚œì´ë„ (0~10)
```

**ê¸°ëŒ€ íš¨ê³¼:** MAE 0.0715 â†’ 0.0650~0.0680 (5~10% ê°œì„ )

---

### 2.4 ì£¼ë³€ í™˜ê²½ ë°ì´í„°

**ê³µê³µ ë°ì´í„° í™œìš©:**

```python
# 1. ë°˜ê²½ 500m ë‚´ ì •ë³´
def get_nearby_info(lat, lon):
    # ê³µê³µ API í˜¸ì¶œ
    return {
        'ì§€í•˜ì² ì—­ê±°ë¦¬': 250,  # ë¯¸í„°
        'í•™êµìˆ˜': 3,
        'ë³‘ì›ìˆ˜': 2,
        'ê³µì›ë©´ì ': 15000,  # ì œê³±ë¯¸í„°
        'ìƒê¶Œí™œì„±ë„': 7.5,  # 0~10
        'ë²”ì£„ìœ¨': 3.2,  # ê±´/ì²œëª…
        'ì†ŒìŒë„': 45  # dB
    }

# 2. í”¼ì²˜ ì¶”ê°€
for idx in df.index:
    lat, lon = df.loc[idx, ['ìœ„ë„', 'ê²½ë„']]
    info = get_nearby_info(lat, lon)
    # ...
```

**ê¸°ëŒ€ íš¨ê³¼:** MAE 0.0715 â†’ 0.0680~0.0700 (2~5% ê°œì„ )

---

## 3. ëª¨ë¸ ê³ ë„í™” (ì¤‘ê¸°)

### 3.1 LightGBM ìµœì í™”

**í˜„ì¬ ì„±ëŠ¥:**
```
LightGBM: MAE 0.0754 (4ìœ„)
```

**ê°œì„  ì „ëµ:**

```python
import optuna

def objective(trial):
    params = {
        'objective': 'regression',
        'metric': 'mae',
        'boosting_type': 'gbdt',
        'num_leaves': trial.suggest_int('num_leaves', 20, 100),
        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),
        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),
        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),
        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),
        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),
    }
    
    model = lgb.LGBMRegressor(**params, n_estimators=1000, random_state=42)
    model.fit(X_train_scaled, y_train,
              eval_set=[(X_val_scaled, y_val)],
              early_stopping_rounds=50,
              verbose=False)
    
    pred = model.predict(X_val_scaled)
    mae = mean_absolute_error(y_val, pred)
    
    return mae

# Optuna ìµœì í™”
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=100)

print(f"Best MAE: {study.best_value:.4f}")
print(f"Best params: {study.best_params}")
```

**ê¸°ëŒ€ íš¨ê³¼:** MAE 0.0754 â†’ 0.0680~0.0700 (7~10% ê°œì„ )

---

### 3.2 XGBoost with Custom Loss

**Custom Loss (Huber Loss):**

```python
import xgboost as xgb

def huber_approx_obj(y_pred, y_true):
    """Huber loss for XGBoost"""
    delta = 1.35
    
    d = y_pred - y_true.get_label()
    grad = np.where(np.abs(d) <= delta, d, delta * np.sign(d))
    hess = np.where(np.abs(d) <= delta, 1.0, 0.0)
    
    return grad, hess

# XGBoost with custom loss
params = {
    'max_depth': 6,
    'learning_rate': 0.05,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'lambda': 1.0,
    'alpha': 0.1
}

dtrain = xgb.DMatrix(X_train_scaled, label=y_train)
dtest = xgb.DMatrix(X_test_scaled, label=y_test)

model = xgb.train(
    params,
    dtrain,
    num_boost_round=1000,
    obj=huber_approx_obj,
    evals=[(dtest, 'test')],
    early_stopping_rounds=50,
    verbose_eval=False
)

pred = model.predict(dtest)
mae = mean_absolute_error(y_test, pred)
print(f"MAE (XGB + Huber): {mae:.4f}")
```

**ê¸°ëŒ€ íš¨ê³¼:** MAE 0.0811 â†’ 0.0700~0.0750 (7~13% ê°œì„ )

---

### 3.3 Neural Network (Tabular)

**êµ¬ì¡°:**

```python
import tensorflow as tf
from tensorflow import keras

# ëª¨ë¸ êµ¬ì¡°
model = keras.Sequential([
    keras.layers.Dense(256, activation='relu', input_shape=(15,)),
    keras.layers.BatchNormalization(),
    keras.layers.Dropout(0.3),
    
    keras.layers.Dense(128, activation='relu'),
    keras.layers.BatchNormalization(),
    keras.layers.Dropout(0.2),
    
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dropout(0.2),
    
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1)
])

# Huber Loss
model.compile(
    optimizer=keras.optimizers.Adam(0.001),
    loss=keras.losses.Huber(delta=1.35)
)

# í•™ìŠµ
history = model.fit(
    X_train_scaled, y_train,
    validation_split=0.2,
    epochs=100,
    batch_size=32,
    callbacks=[
        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),
        keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)
    ],
    verbose=0
)

# ì˜ˆì¸¡
pred = model.predict(X_test_scaled).flatten()
mae = mean_absolute_error(y_test, pred)
print(f"MAE (Neural Net): {mae:.4f}")
```

**ê¸°ëŒ€ íš¨ê³¼:** MAE 0.0812 â†’ 0.0700~0.0750 (7~13% ê°œì„ )

---

## 4. ì•™ìƒë¸” ì „ëµ (ì¤‘ê¸°)

### 4.1 Stacking

**êµ¬ì¡°:**

```python
from sklearn.ensemble import StackingRegressor

# Base models
base_models = [
    ('huber', HuberRegressor(epsilon=1.35, alpha=0.0001)),
    ('lgbm', lgb.LGBMRegressor(n_estimators=1000)),
    ('xgb', xgb.XGBRegressor(n_estimators=1000)),
    ('catboost', CatBoostRegressor(iterations=3000, verbose=False))
]

# Meta model
meta_model = HuberRegressor(epsilon=1.35, alpha=0.0001)

# Stacking
stacking = StackingRegressor(
    estimators=base_models,
    final_estimator=meta_model,
    cv=5
)

stacking.fit(X_train_scaled, y_train)
pred = stacking.predict(X_test_scaled)

mae = mean_absolute_error(y_test, pred)
print(f"MAE (Stacking): {mae:.4f}")
```

**ê¸°ëŒ€ íš¨ê³¼:** MAE 0.0715 â†’ 0.0650~0.0680 (5~10% ê°œì„ )

---

### 4.2 Weighted Ensemble (ë‹¤ìˆ˜ ëª¨ë¸)

**ì „ëµ:**

```python
# 5ê°œ ëª¨ë¸ ì•™ìƒë¸”
models = {
    'huber_15': huber_15_pred,
    'lgbm_opt': lgbm_opt_pred,
    'xgb_huber': xgb_huber_pred,
    'catboost': catboost_pred,
    'nn': nn_pred
}

# Optunaë¡œ ê°€ì¤‘ì¹˜ ìµœì í™”
def objective(trial):
    weights = [
        trial.suggest_float(f'w{i}', 0, 1) 
        for i in range(len(models))
    ]
    
    # ì •ê·œí™”
    total = sum(weights)
    weights = [w / total for w in weights]
    
    # ì•™ìƒë¸”
    pred = sum(w * p for w, p in zip(weights, models.values()))
    
    mae = mean_absolute_error(y_test, pred)
    return mae

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=500)

print(f"Best MAE: {study.best_value:.4f}")
print(f"Best weights: {study.best_params}")
```

**ê¸°ëŒ€ íš¨ê³¼:** MAE 0.0715 â†’ 0.0630~0.0660 (7~12% ê°œì„ )

---

## 5. ì·¨ì•½ êµ¬ê°„ ì§‘ì¤‘ (ì¤‘ê¸°)

### 5.1 2ë‹¨ê³„ ëª¨ë¸ (ë¶„ë¥˜ â†’ íšŒê·€)

**ì „ëµ:**

```python
# Stage 1: ë¶„ë¥˜ (ìœ ì°° vs ë‚™ì°°)
from sklearn.ensemble import RandomForestClassifier

# ìœ ì°° = ë‚™ì°°ê°€ìœ¨ < 0.5
y_class = (y_train < 0.5).astype(int)

clf = RandomForestClassifier(n_estimators=500, random_state=42)
clf.fit(X_train_scaled, y_class)

# Stage 2: ê° ê·¸ë£¹ë³„ íšŒê·€
# ìœ ì°° ê·¸ë£¹
fail_mask_train = (clf.predict(X_train_scaled) == 1)
X_fail = X_train_scaled[fail_mask_train]
y_fail = y_train[fail_mask_train]

model_fail = HuberRegressor(epsilon=1.35, alpha=0.0001)
model_fail.fit(X_fail, y_fail)

# ë‚™ì°° ê·¸ë£¹
success_mask_train = (clf.predict(X_train_scaled) == 0)
X_success = X_train_scaled[success_mask_train]
y_success = y_train[success_mask_train]

model_success = HuberRegressor(epsilon=1.35, alpha=0.0001)
model_success.fit(X_success, y_success)

# ì˜ˆì¸¡
class_pred = clf.predict(X_test_scaled)

pred = np.zeros(len(X_test_scaled))
pred[class_pred == 0] = model_success.predict(X_test_scaled[class_pred == 0])
pred[class_pred == 1] = model_fail.predict(X_test_scaled[class_pred == 1])

mae = mean_absolute_error(y_test, pred)
print(f"MAE (2-stage): {mae:.4f}")

# ì €ê°€ êµ¬ê°„ë§Œ
low_mask = (y_test < 0.5)
mae_low = mean_absolute_error(y_test[low_mask], pred[low_mask])
print(f"MAE (low price): {mae_low:.4f}")
```

**ê¸°ëŒ€ íš¨ê³¼:** 
- ì „ì²´ MAE: 0.0715 â†’ 0.0680 (~5% ê°œì„ )
- ì €ê°€ MAE: 0.0805 â†’ 0.0650 (~19% ê°œì„ !)

---

### 5.2 SMOTE (ì˜¤ë²„ìƒ˜í”Œë§)

**ì „ëµ:**

```python
from imblearn.over_sampling import SMOTE

# ì €ê°€ ì¼€ì´ìŠ¤ ì˜¤ë²„ìƒ˜í”Œë§
low_price_indices = (y_train < 0.5)

# ê· í˜• ë§ì¶”ê¸°
smote = SMOTE(sampling_strategy=0.3, random_state=42)
X_resampled, y_resampled = smote.fit_resample(
    X_train_scaled, 
    low_price_indices.astype(int)
)

# íƒ€ê²Ÿë„ ë³µì œ
y_train_resampled = np.concatenate([
    y_train,
    y_train[low_price_indices][:(len(y_resampled) - len(y_train))]
])

# í•™ìŠµ
model = HuberRegressor(epsilon=1.35, alpha=0.0001)
model.fit(X_resampled, y_train_resampled)

pred = model.predict(X_test_scaled)
mae = mean_absolute_error(y_test, pred)

# ì €ê°€ êµ¬ê°„
mae_low = mean_absolute_error(y_test[y_test < 0.5], 
                               pred[y_test < 0.5])
print(f"MAE (ì „ì²´): {mae:.4f}")
print(f"MAE (ì €ê°€): {mae_low:.4f}")
```

**ê¸°ëŒ€ íš¨ê³¼:** ì €ê°€ MAE 0.0805 â†’ 0.0700 (~13% ê°œì„ )

---

### 5.3 Sample Weighting

**ì „ëµ:**

```python
# ì €ê°€ ì¼€ì´ìŠ¤ì— ë†’ì€ ê°€ì¤‘ì¹˜
sample_weights = np.ones(len(y_train))
sample_weights[y_train < 0.5] = 5.0  # 5ë°° ê°€ì¤‘ì¹˜

# LightGBM (ê°€ì¤‘ì¹˜ ì§€ì›)
model = lgb.LGBMRegressor(n_estimators=1000)
model.fit(
    X_train_scaled, 
    y_train,
    sample_weight=sample_weights
)

pred = model.predict(X_test_scaled)
mae = mean_absolute_error(y_test, pred)

# ì €ê°€ êµ¬ê°„
mae_low = mean_absolute_error(y_test[y_test < 0.5], 
                               pred[y_test < 0.5])
print(f"MAE (ì „ì²´): {mae:.4f}")
print(f"MAE (ì €ê°€): {mae_low:.4f}")
```

**ê¸°ëŒ€ íš¨ê³¼:** ì €ê°€ MAE 0.0805 â†’ 0.0720 (~10% ê°œì„ )

---

## 6. ì¥ê¸° ì „ëµ

### 6.1 ë” ë§ì€ ë°ì´í„°

**í˜„ì¬:**
```
17,569ê±´ (2020~2024)
```

**í™•ì¥:**
```python
# 1. ê¸°ê°„ í™•ì¥
# 2010~2024 â†’ 40,000ê±´+

# 2. ì§€ì—­ í™•ì¥
# ì„œìš¸ â†’ ìˆ˜ë„ê¶Œ
# ì„œìš¸ â†’ ì „êµ­

# 3. ìœ í˜• í™•ì¥
# ì•„íŒŒíŠ¸ â†’ ìƒê°€, í† ì§€, ì˜¤í”¼ìŠ¤í…”
```

**ê¸°ëŒ€ íš¨ê³¼:** 
- ë°ì´í„° 3ë°° â†’ MAE 10~15% ê°œì„ 
- ì €ê°€ ì¼€ì´ìŠ¤ ì¦ê°€ â†’ ì €ê°€ MAE 20% ê°œì„ 

---

### 6.2 ì‹¤ì‹œê°„ ë°ì´í„°

**ì¶”ê°€ ì •ë³´:**
```
ì‹¤ì‹œê°„ ì •ë³´:
- í˜„ì¬ ì‹œì¥ ì˜¨ë„ (HOT/WARM/COLD)
- ìµœê·¼ 1ê°œì›” ë‚™ì°°ê°€ìœ¨ í‰ê· 
- ë™ì¼ ì§€ì—­ ê²½ìŸ ë¬¼ê±´ ìˆ˜
- ê¸ˆë¦¬ ì¶”ì´
- ë¶€ë™ì‚° ë‰´ìŠ¤ ê°ì„± ë¶„ì„
```

**êµ¬í˜„:**

```python
# 1. ë§¤í¬ë¡œ ì§€í‘œ
def get_market_condition(date):
    # API í˜¸ì¶œ
    return {
        'ê¸ˆë¦¬': 3.5,
        'ì‹œì¥ì˜¨ë„': 7.5,  # 0~10
        'ìµœê·¼_í‰ê· ë‚™ì°°ë¥ ': 0.78,
        'ê²½ìŸë¬¼ê±´ìˆ˜': 5
    }

# 2. ë‰´ìŠ¤ ê°ì„±
from transformers import pipeline

sentiment = pipeline("sentiment-analysis", 
                     model="beomi/kcbert-base")

news_score = sentiment("ë¶€ë™ì‚° ì‹œì¥ í™œí™©...")

# 3. í”¼ì²˜ ì¶”ê°€
# ...
```

**ê¸°ëŒ€ íš¨ê³¼:** MAE 0.0715 â†’ 0.0650 (~10% ê°œì„ )

---

### 6.3 ê°•í™”í•™ìŠµ (Advanced)

**ê°œë…:**
```
ì—ì´ì „íŠ¸ê°€ ì˜ˆì¸¡ â†’ ë³´ìƒ â†’ í•™ìŠµ
â†’ ë™ì  ìµœì í™”
```

**êµ¬í˜„ (ê°œë…):**

```python
import gym
from stable_baselines3 import PPO

# í™˜ê²½ ì •ì˜
class AuctionEnv(gym.Env):
    def __init__(self):
        # ìƒíƒœ: í”¼ì²˜
        # ì•¡ì…˜: ë‚™ì°°ê°€ìœ¨ ì˜ˆì¸¡
        # ë³´ìƒ: -MAE
        pass
    
    def step(self, action):
        # ì˜ˆì¸¡ â†’ ë³´ìƒ
        reward = -abs(action - true_value)
        return next_state, reward, done, info

# í•™ìŠµ
env = AuctionEnv()
model = PPO("MlpPolicy", env, verbose=1)
model.learn(total_timesteps=100000)

# ì˜ˆì¸¡
# ...
```

**ê¸°ëŒ€ íš¨ê³¼:** ì‹¤í—˜ì  (ë¶ˆí™•ì‹¤)

---

## ğŸ“Š ê°œì„  ì „ëµ ë¡œë“œë§µ

### Phase 1 (1ì£¼)
```
âœ… Huber í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
âœ… íƒ€ê²Ÿ ë³€í™˜ (Log)
âœ… ì´ìƒì¹˜ ì œê±°

ëª©í‘œ: MAE 0.0715 â†’ 0.0650 (~10% ê°œì„ )
```

### Phase 2 (2ì£¼)
```
âœ… 2ë‹¨ê³„ ëª¨ë¸ (ë¶„ë¥˜ â†’ íšŒê·€)
âœ… Sample Weighting
âœ… LightGBM ìµœì í™”

ëª©í‘œ: 
- ì „ì²´ MAE 0.0650 â†’ 0.0620
- ì €ê°€ MAE 0.0805 â†’ 0.0650
```

### Phase 3 (1ê°œì›”)
```
âœ… ì´ë¯¸ì§€ ë°ì´í„° ì¶”ê°€ (í¬ë¡¤ë§ ì™„ë£Œ ì‹œ)
âœ… Stacking ì•™ìƒë¸”
âœ… ë“±ê¸°ë¶€/ì„ì°¨ì¸ ì •ë³´

ëª©í‘œ: MAE 0.0620 â†’ 0.0500~0.0550 (20~30% ê°œì„ )
```

### Phase 4 (ì¥ê¸°)
```
âœ… ë°ì´í„° í™•ì¥ (ìˆ˜ë„ê¶Œ, ê¸°ê°„)
âœ… ì‹¤ì‹œê°„ ì •ë³´
âœ… Neural Network ê³ ë„í™”

ëª©í‘œ: 
- MAE < 0.0500
- Within 5%p > 70%
```

---

## ğŸ¯ ìš°ì„ ìˆœìœ„ (ë‹¹ì¥ í•  ê²ƒ)

### TOP 3 (ì¦‰ì‹œ ì‹¤í–‰)

#### 1ìœ„: íƒ€ê²Ÿ ë³€í™˜ (Log Transform)
```
íˆ¬ì…: 30ë¶„
íš¨ê³¼: 9% ê°œì„ 
ROI: â­â­â­â­â­
```

#### 2ìœ„: ì´ìƒì¹˜ ì œê±°
```
íˆ¬ì…: 1ì‹œê°„
íš¨ê³¼: 5% ê°œì„ 
ROI: â­â­â­â­â­
```

#### 3ìœ„: 2ë‹¨ê³„ ëª¨ë¸
```
íˆ¬ì…: 2ì‹œê°„
íš¨ê³¼: ì €ê°€ êµ¬ê°„ 19% ê°œì„ 
ROI: â­â­â­â­â­
```

---

## ğŸ’¡ ì‹¤í–‰ ì½”ë“œ

### í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

```python
# ============================================================
# ğŸš€ ì„±ëŠ¥ ê°œì„  í†µí•© ìŠ¤í¬ë¦½íŠ¸
# ============================================================

print("=" * 80)
print("ğŸš€ ì„±ëŠ¥ ê°œì„  ì‹¤í—˜")
print("=" * 80)

# ============================================================
# [1] Baseline í™•ì¸
# ============================================================

print("\n[1] Baseline")
print(f"   Ensemble: MAE {best_ensemble_mae:.4f}")
print(f"   Within 5%p: 49.7%")

# ============================================================
# [2] íƒ€ê²Ÿ ë³€í™˜ (Log)
# ============================================================

print("\n[2] Log Transform")

# Log ë³€í™˜
y_train_log = np.log1p(y_train_array)
y_test_log = np.log1p(y_test_array)

# í•™ìŠµ
model_log = HuberRegressor(epsilon=1.35, alpha=0.0001, max_iter=100)
model_log.fit(X_train_scaled, y_train_log)

# ì˜ˆì¸¡ (ì—­ë³€í™˜)
pred_log = model_log.predict(X_test_scaled)
pred_log_inv = np.expm1(pred_log)

# í‰ê°€
mae_log = mean_absolute_error(y_test_array, pred_log_inv)
print(f"   MAE: {mae_log:.4f}")
print(f"   ê°œì„ : {(huber_test_mae - mae_log) / huber_test_mae * 100:+.1f}%")

# ============================================================
# [3] ì´ìƒì¹˜ ì œê±°
# ============================================================

print("\n[3] ì´ìƒì¹˜ ì œê±°")

from sklearn.ensemble import IsolationForest

# ì´ìƒì¹˜ íƒì§€
iso = IsolationForest(contamination=0.05, random_state=42)
outliers = iso.fit_predict(X_train_scaled)

# ì •ìƒ ë°ì´í„°
X_train_clean = X_train_scaled[outliers == 1]
y_train_clean = y_train_array[outliers == 1]

print(f"   ì œê±°: {(outliers == -1).sum()}ê°œ")
print(f"   ë‚¨ìŒ: {len(X_train_clean)}ê°œ")

# í•™ìŠµ
model_clean = HuberRegressor(epsilon=1.35, alpha=0.0001, max_iter=100)
model_clean.fit(X_train_clean, y_train_clean)

# ì˜ˆì¸¡
pred_clean = model_clean.predict(X_test_scaled)

# í‰ê°€
mae_clean = mean_absolute_error(y_test_array, pred_clean)
print(f"   MAE: {mae_clean:.4f}")
print(f"   ê°œì„ : {(huber_test_mae - mae_clean) / huber_test_mae * 100:+.1f}%")

# ============================================================
# [4] 2ë‹¨ê³„ ëª¨ë¸
# ============================================================

print("\n[4] 2ë‹¨ê³„ ëª¨ë¸ (ë¶„ë¥˜ â†’ íšŒê·€)")

from sklearn.ensemble import RandomForestClassifier

# Stage 1: ë¶„ë¥˜
y_class_train = (y_train_array < 0.5).astype(int)

clf = RandomForestClassifier(n_estimators=500, random_state=42, 
                              max_depth=10, min_samples_split=20)
clf.fit(X_train_scaled, y_class_train)

# Stage 2: ê·¸ë£¹ë³„ íšŒê·€
fail_mask = (y_class_train == 1)
success_mask = (y_class_train == 0)

# ìœ ì°° ëª¨ë¸
model_fail = HuberRegressor(epsilon=1.35, alpha=0.0001, max_iter=100)
model_fail.fit(X_train_scaled[fail_mask], y_train_array[fail_mask])

# ë‚™ì°° ëª¨ë¸
model_success = HuberRegressor(epsilon=1.35, alpha=0.0001, max_iter=100)
model_success.fit(X_train_scaled[success_mask], y_train_array[success_mask])

# ì˜ˆì¸¡
class_pred = clf.predict(X_test_scaled)

pred_2stage = np.zeros(len(X_test_scaled))
pred_2stage[class_pred == 0] = model_success.predict(
    X_test_scaled[class_pred == 0]
)
pred_2stage[class_pred == 1] = model_fail.predict(
    X_test_scaled[class_pred == 1]
)

# í‰ê°€ (ì „ì²´)
mae_2stage = mean_absolute_error(y_test_array, pred_2stage)
print(f"   MAE (ì „ì²´): {mae_2stage:.4f}")
print(f"   ê°œì„ : {(huber_test_mae - mae_2stage) / huber_test_mae * 100:+.1f}%")

# í‰ê°€ (ì €ê°€)
low_mask = (y_test_array < 0.5)
mae_2stage_low = mean_absolute_error(y_test_array[low_mask], 
                                      pred_2stage[low_mask])
mae_baseline_low = 0.0805
print(f"   MAE (ì €ê°€): {mae_2stage_low:.4f}")
print(f"   ê°œì„ : {(mae_baseline_low - mae_2stage_low) / mae_baseline_low * 100:+.1f}%")

# ============================================================
# [5] ìµœì¢… ë¹„êµ
# ============================================================

print("\n" + "=" * 80)
print("ğŸ“Š ìµœì¢… ë¹„êµ")
print("=" * 80)

results = pd.DataFrame({
    'Method': [
        'Baseline (Ensemble)',
        'Baseline (Huber)',
        'Log Transform',
        'Outlier Removal',
        '2-Stage Model'
    ],
    'MAE': [
        best_ensemble_mae,
        huber_test_mae,
        mae_log,
        mae_clean,
        mae_2stage
    ],
    'Improvement': [
        0,
        (best_ensemble_mae - huber_test_mae) / best_ensemble_mae * 100,
        (huber_test_mae - mae_log) / huber_test_mae * 100,
        (huber_test_mae - mae_clean) / huber_test_mae * 100,
        (huber_test_mae - mae_2stage) / huber_test_mae * 100
    ]
}).sort_values('MAE')

print("\n")
print(results.to_string(index=False))

best = results.iloc[0]
print(f"\nğŸ† ìµœê³  ì„±ëŠ¥:")
print(f"   ë°©ë²•: {best['Method']}")
print(f"   MAE: {best['MAE']:.4f}")
print(f"   ê°œì„ : {best['Improvement']:+.1f}%")

# Within 5%p (ìµœê³  ëª¨ë¸)
best_pred = eval(best['Method'].split()[0].lower() + '_pred' 
                 if 'Baseline' in best['Method'] 
                 else 'pred_' + best['Method'].split()[0].lower())

abs_errors = np.abs(best_pred - y_test_array)
within_5p = (abs_errors <= 0.05).sum() / len(abs_errors) * 100

print(f"   Within 5%p: {within_5p:.1f}%")

print("\n" + "=" * 80)
```

---

**ë¬¸ì„œ ì‘ì„±:** 2025.01.26  
**ë²„ì „:** 1.0  
**í”„ë¡œì íŠ¸:** ì„œìš¸ ê²½ë§¤ ë‚™ì°°ê°€ìœ¨ ì˜ˆì¸¡ - ì„±ëŠ¥ ê°œì„ 

---

*ì´ ë¬¸ì„œëŠ” í˜„ì¬ MAE 0.0715ì—ì„œ 0.0500~0.0550 ìˆ˜ì¤€ìœ¼ë¡œ ê°œì„ í•˜ê¸° ìœ„í•œ ì¢…í•© ì „ëµì…ë‹ˆë‹¤.*
