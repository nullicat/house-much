# ============================================================
# ğŸ¯ ê°„ë‹¨í•œ MLP í•™ìŠµ
# ============================================================

print("=" * 80)
print("ğŸ¯ ê°„ë‹¨í•œ MLP í•™ìŠµ")
print("=" * 80)

# í•™ìŠµ í•¨ìˆ˜ (ê°„ë‹¨ ë²„ì „)
def train_simple_epoch(model, loader, criterion, optimizer, device):
    """1 ì—í­ í•™ìŠµ"""
    model.train()
    total_loss = 0

    for X, y in loader:
        X, y = X.to(device), y.to(device)

        optimizer.zero_grad()
        output = model(X)
        loss = criterion(output, y)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(loader)

# í‰ê°€ í•¨ìˆ˜ (ê°„ë‹¨ ë²„ì „)
def evaluate_simple(model, loader, device):
    """í‰ê°€"""
    model.eval()
    predictions = []
    targets = []

    with torch.no_grad():
        for X, y in loader:
            X = X.to(device)
            output = model(X)

            predictions.extend(output.cpu().numpy())
            targets.extend(y.numpy())

    predictions = np.array(predictions)
    targets = np.array(targets)

    mae = mean_absolute_error(targets, predictions)
    rmse = np.sqrt(mean_squared_error(targets, predictions))
    r2 = r2_score(targets, predictions)

    return mae, rmse, r2, predictions

# ============================================================
# í•™ìŠµ ì‹œì‘
# ============================================================

print("\ní•™ìŠµ ì‹œì‘!")

num_epochs = 100
best_simple_mae = float('inf')
patience_counter = 0
early_stop_patience = 15

simple_history = {
    'train_loss': [],
    'val_mae': [],
    'val_rmse': [],
    'val_r2': []
}

print(f"Epochs: {num_epochs}")
print(f"Early Stopping: {early_stop_patience}")
print("-" * 80)

for epoch in range(num_epochs):
    # í•™ìŠµ
    train_loss = train_simple_epoch(
        simple_model, simple_train_loader,
        simple_criterion, simple_optimizer, device
    )

    # í‰ê°€
    val_mae, val_rmse, val_r2, _ = evaluate_simple(
        simple_model, simple_test_loader, device
    )

    # ê¸°ë¡
    simple_history['train_loss'].append(train_loss)
    simple_history['val_mae'].append(val_mae)
    simple_history['val_rmse'].append(val_rmse)
    simple_history['val_r2'].append(val_r2)

    # ìŠ¤ì¼€ì¤„ëŸ¬
    simple_scheduler.step(val_mae)

    # ì¶œë ¥
    if (epoch + 1) % 10 == 0 or epoch == 0:
        print(f"Epoch {epoch+1:3d}/{num_epochs} | "
              f"Loss: {train_loss:.4f} | "
              f"MAE: {val_mae:.4f} | "
              f"RMSE: {val_rmse:.4f} | "
              f"RÂ²: {val_r2:.4f}")

    # Best ëª¨ë¸ ì €ì¥
    if val_mae < best_simple_mae:
        best_simple_mae = val_mae
        best_simple_epoch = epoch + 1
        patience_counter = 0

        torch.save(simple_model.state_dict(),
                   f'{backup_dir}/simple_mlp_best.pth')
    else:
        patience_counter += 1

    # Early Stopping
    if patience_counter >= early_stop_patience:
        print(f"\nEarly Stopping at Epoch {epoch+1}")
        break

print("\n" + "=" * 80)
print("âœ… í•™ìŠµ ì™„ë£Œ!")
print("=" * 80)

print(f"""
Best ì„±ëŠ¥:
- Epoch: {best_simple_epoch}
- MAE: {best_simple_mae:.4f}
""")

# ============================================================
# ìµœì¢… í‰ê°€
# ============================================================

print("\n[ìµœì¢… í‰ê°€]")

# Best ëª¨ë¸ ë¡œë“œ
simple_model.load_state_dict(
    torch.load(f'{backup_dir}/simple_mlp_best.pth')
)

# í‰ê°€
simple_mae, simple_rmse, simple_r2, simple_preds = evaluate_simple(
    simple_model, simple_test_loader, device
)

print(f"\nê°„ë‹¨í•œ MLP ì„±ëŠ¥:")
print(f"   MAE:  {simple_mae:.4f}")
print(f"   RMSE: {simple_rmse:.4f}")
print(f"   RÂ²:   {simple_r2:.4f}")

# ============================================================
# ì „ì²´ ë¹„êµ
# ============================================================

print("\n" + "=" * 80)
print("ğŸ“Š ì „ì²´ ëª¨ë¸ ë¹„êµ")
print("=" * 80)

results_dl = pd.DataFrame({
    'Model': ['CatBoost', 'Multimodal', 'Simple MLP'],
    'MAE': [0.0747, final_mae, simple_mae],
    'RMSE': [0.1173, final_rmse, simple_rmse],
    'RÂ²': [0.6710, final_r2, simple_r2],
    'Features': ['ìˆ˜ì¹˜ 10ê°œ', 'ìˆ˜ì¹˜ 24ê°œ + í…ìŠ¤íŠ¸', 'ìˆ˜ì¹˜ 24ê°œ'],
    'Parameters': ['-', f'{total_params:,}', f'{simple_params:,}']
})

results_dl = results_dl.sort_values('MAE')

print("\n")
print(results_dl.to_string(index=False))

print("\nğŸ† ìµœê³  ì„±ëŠ¥:")
best_model = results_dl.iloc[0]
print(f"   ëª¨ë¸: {best_model['Model']}")
print(f"   MAE: {best_model['MAE']:.4f}")

# Simple MLP vs CatBoost
improvement = (0.0747 - simple_mae) / 0.0747 * 100

print(f"\n[Simple MLP vs CatBoost]")
if simple_mae < 0.0747:
    print(f"   âœ… Simple MLPê°€ {improvement:.2f}% ë” ì¢‹ìŒ!")
elif simple_mae < 0.0750:
    print(f"   â†’ ê±°ì˜ ë¹„ìŠ·í•¨ (ì°¨ì´ {abs(improvement):.2f}%)")
else:
    print(f"   â†’ CatBoostê°€ ì—¬ì „íˆ ìš°ìˆ˜")

print(f"\nëª¨ë¸ ì €ì¥ ì™„ë£Œ:")
print(f"   {backup_dir}/simple_mlp_best.pth")