# ============================================================
# ğŸ§  PyTorch Multimodal ëª¨ë¸ êµ¬ì¶•
# ============================================================

print("=" * 80)
print("ğŸ§  PyTorch Multimodal ëª¨ë¸ êµ¬ì¶•")
print("=" * 80)

print("""
ğŸ’¡ ëª¨ë¸ êµ¬ì¡°:

Branch 1: ìˆ˜ì¹˜ (24ê°œ í”¼ì²˜)
   Input(24) â†’ FC(64) â†’ ReLU â†’ Dropout
   â†’ FC(32) â†’ ReLU

Branch 2: í…ìŠ¤íŠ¸ (768ì°¨ì› BERT)
   Input(768) â†’ FC(256) â†’ ReLU â†’ Dropout
   â†’ FC(128) â†’ ReLU â†’ Dropout
   â†’ FC(32) â†’ ReLU

Fusion:
   Concat(32 + 32 = 64)
   â†’ FC(32) â†’ ReLU
   â†’ FC(1) â†’ Output
""")

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# ============================================================
# [1] Dataset í´ë˜ìŠ¤
# ============================================================

print("\n[1] Dataset í´ë˜ìŠ¤ ì •ì˜")

class AuctionDataset(Dataset):
    """ê²½ë§¤ ë°ì´í„°ì…‹"""

    def __init__(self, X_numeric, X_bert, y):
        self.X_numeric = torch.FloatTensor(X_numeric)
        self.X_bert = torch.FloatTensor(X_bert)
        self.y = torch.FloatTensor(y)

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        return {
            'numeric': self.X_numeric[idx],
            'bert': self.X_bert[idx],
            'target': self.y[idx]
        }

# Dataset ìƒì„±
train_dataset = AuctionDataset(X_train_numeric, X_train_bert, y_train_dl)
test_dataset = AuctionDataset(X_test_numeric, X_test_bert, y_test_dl)

print(f"   Train Dataset: {len(train_dataset)}ê°œ")
print(f"   Test Dataset: {len(test_dataset)}ê°œ")

# DataLoader ìƒì„±
batch_size = 64

train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=2
)

test_loader = DataLoader(
    test_dataset,
    batch_size=batch_size,
    shuffle=False,
    num_workers=2
)

print(f"   Batch size: {batch_size}")
print(f"   Train batches: {len(train_loader)}")
print(f"   Test batches: {len(test_loader)}")

# ============================================================
# [2] ëª¨ë¸ í´ë˜ìŠ¤
# ============================================================

print("\n[2] Multimodal ëª¨ë¸ ì •ì˜")

class MultimodalModel(nn.Module):
    """ìˆ˜ì¹˜ + í…ìŠ¤íŠ¸ Multimodal ëª¨ë¸"""

    def __init__(self, numeric_dim=24, bert_dim=768):
        super().__init__()

        # Branch 1: ìˆ˜ì¹˜ í”¼ì²˜
        self.numeric_branch = nn.Sequential(
            nn.Linear(numeric_dim, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.3),

            nn.Linear(64, 32),
            nn.BatchNorm1d(32),
            nn.ReLU()
        )

        # Branch 2: BERT ì„ë² ë”©
        self.bert_branch = nn.Sequential(
            nn.Linear(bert_dim, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.4),

            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3),

            nn.Linear(128, 32),
            nn.BatchNorm1d(32),
            nn.ReLU()
        )

        # Fusion Layer
        self.fusion = nn.Sequential(
            nn.Linear(64, 32),  # 32 + 32 = 64
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(0.2),

            nn.Linear(32, 1)
        )

    def forward(self, numeric, bert):
        # ê° Branch í†µê³¼
        numeric_out = self.numeric_branch(numeric)
        bert_out = self.bert_branch(bert)

        # ê²°í•©
        combined = torch.cat([numeric_out, bert_out], dim=1)

        # ìµœì¢… ì˜ˆì¸¡
        output = self.fusion(combined)

        return output.squeeze()

# ëª¨ë¸ ìƒì„±
model = MultimodalModel(
    numeric_dim=X_train_numeric.shape[1],
    bert_dim=X_train_bert.shape[1]
)

model = model.to(device)

# íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"   âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ")
print(f"   ì´ íŒŒë¼ë¯¸í„°: {total_params:,}ê°œ")
print(f"   í•™ìŠµ ê°€ëŠ¥: {trainable_params:,}ê°œ")

# ============================================================
# [3] ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €
# ============================================================

print("\n[3] í•™ìŠµ ì„¤ì •")

criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='min',
    factor=0.5,
    patience=5
)

print(f"   ì†ì‹¤ í•¨ìˆ˜: MSE")
print(f"   ì˜µí‹°ë§ˆì´ì €: Adam")
print(f"   í•™ìŠµë¥ : 0.001")
print(f"   ìŠ¤ì¼€ì¤„ëŸ¬: ReduceLROnPlateau")

# ============================================================
# ì™„ë£Œ
# ============================================================

print("\n" + "=" * 80)
print("âœ… ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ!")
print("=" * 80)

print(f"""
ì¤€ë¹„ëœ ê²ƒ:
- ëª¨ë¸: Multimodal ({total_params:,} íŒŒë¼ë¯¸í„°)
- ë°ì´í„°: Train {len(train_dataset):,}ê°œ, Test {len(test_dataset):,}ê°œ
- ë””ë°”ì´ìŠ¤: {device}

ë‹¤ìŒ ë‹¨ê³„: í•™ìŠµ ì‹œì‘!
""")