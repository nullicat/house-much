# ============================================================
# STEP 8: ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ (Naive Mean)
# ============================================================

print("\n" + "=" * 80)
print("ğŸ“Š STEP 8: ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ (Naive Mean)")
print("=" * 80)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# ë² ì´ìŠ¤ë¼ì¸ ì˜ˆì¸¡: Train í‰ê· ê°’
baseline_pred = np.full(len(y_test), y_train.mean())

# í‰ê°€
mae_baseline = mean_absolute_error(y_test, baseline_pred)
rmse_baseline = np.sqrt(mean_squared_error(y_test, baseline_pred))
r2_baseline = r2_score(y_test, baseline_pred)

print(f"\në² ì´ìŠ¤ë¼ì¸ (í‰ê·  ì˜ˆì¸¡: {y_train.mean():.3f})")
print(f"   - MAE:  {mae_baseline:.4f}")
print(f"   - RMSE: {rmse_baseline:.4f}")
print(f"   - RÂ²:   {r2_baseline:.4f}")

print("\nğŸ’¬ í•´ì„:")
print(f"   ëª¨ë“  ë¬¼ê±´ì„ í‰ê·  {y_train.mean()*100:.1f}%ë¡œ ì˜ˆì¸¡í•˜ë©´")
print(f"   í‰ê·  {mae_baseline*100:.2f}%p ì˜¤ì°¨ ë°œìƒ")

# ============================================================
# STEP 9: Linear Regression (ì„ í˜• íšŒê·€)
# ============================================================

print("\n" + "=" * 80)
print("ğŸ“ˆ STEP 9: Linear Regression (ì„ í˜• íšŒê·€)")
print("=" * 80)

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

# ìŠ¤ì¼€ì¼ë§ (ì„ í˜• ëª¨ë¸ì€ ìŠ¤ì¼€ì¼ì— ë¯¼ê°)
print("\n[9-1] í”¼ì²˜ ìŠ¤ì¼€ì¼ë§")
scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("   âœ… StandardScaler ì ìš© ì™„ë£Œ")

# Linear Regression ëª¨ë¸ í•™ìŠµ
print("\n[9-2] Linear Regression ëª¨ë¸ í•™ìŠµ")

linear = LinearRegression()
linear.fit(X_train_scaled, y_train)

print("   âœ… í•™ìŠµ ì™„ë£Œ")

# ì˜ˆì¸¡
y_pred_linear = linear.predict(X_test_scaled)

# í‰ê°€
mae_linear = mean_absolute_error(y_test, y_pred_linear)
rmse_linear = np.sqrt(mean_squared_error(y_test, y_pred_linear))
r2_linear = r2_score(y_test, y_pred_linear)

print(f"\n[9-3] Linear Regression ì„±ëŠ¥")
print(f"   - MAE:  {mae_linear:.4f}")
print(f"   - RMSE: {rmse_linear:.4f}")
print(f"   - RÂ²:   {r2_linear:.4f}")

print("\nğŸ’¬ í•´ì„:")
print(f"   í‰ê·  {mae_linear*100:.2f}%p ì˜¤ì°¨ë¡œ ì˜ˆì¸¡")
print(f"   RÂ² {r2_linear:.3f} = ì „ì²´ ë¶„ì‚°ì˜ {r2_linear*100:.1f}% ì„¤ëª…")

# íšŒê·€ ìˆ˜ì‹ í™•ì¸
print(f"\n[9-4] íšŒê·€ ìˆ˜ì‹ (y = aâ‚xâ‚ + aâ‚‚xâ‚‚ + ... + b)")
print(f"   - ê³„ìˆ˜(coefficient) ê°œìˆ˜: {len(linear.coef_)}ê°œ")
print(f"   - ì ˆí¸(intercept): {linear.intercept_:.3f}")

# ============================================================
# STEP 10: ë² ì´ìŠ¤ë¼ì¸ vs Linear Regression ë¹„êµ
# ============================================================

print("\n" + "=" * 80)
print("ğŸ“Š STEP 10: ì„±ëŠ¥ ë¹„êµ")
print("=" * 80)

comparison = pd.DataFrame({
    'Model': ['Baseline (í‰ê· )', 'Linear Regression'],
    'MAE': [mae_baseline, mae_linear],
    'RMSE': [rmse_baseline, rmse_linear],
    'RÂ²': [r2_baseline, r2_linear]
})

print("\nì„±ëŠ¥ ë¹„êµí‘œ:")
display(comparison)

# ê°œì„ ìœ¨
improvement_mae = (mae_baseline - mae_linear) / mae_baseline * 100
improvement_rmse = (rmse_baseline - rmse_linear) / rmse_baseline * 100

print(f"\nê°œì„ ìœ¨:")
print(f"   - MAE:  {improvement_mae:.1f}% ê°œì„  âœ…")
print(f"   - RMSE: {improvement_rmse:.1f}% ê°œì„  âœ…")

# ============================================================
# STEP 11: í†µê³„ì  ê²€ì • (H1: Linear vs Baseline)
# ============================================================

print("\n" + "=" * 80)
print("ğŸ“Š STEP 11: í†µê³„ì  ê²€ì • (H1: Linear Regression vs Baseline)")
print("=" * 80)

from scipy.stats import ttest_rel

# ì”ì°¨ ê³„ì‚°
residuals_baseline = np.abs(y_test - baseline_pred)
residuals_linear = np.abs(y_test - y_pred_linear)

# Paired t-test
t_stat, p_value = ttest_rel(residuals_baseline, residuals_linear)

# Cohen's d (íš¨ê³¼ í¬ê¸°)
def cohens_d(group1, group2):
    n1, n2 = len(group1), len(group2)
    var1 = np.var(group1, ddof=1)
    var2 = np.var(group2, ddof=1)
    pooled_std = np.sqrt(((n1-1)*var1 + (n2-1)*var2) / (n1+n2-2))
    return (np.mean(group1) - np.mean(group2)) / pooled_std

d = cohens_d(residuals_baseline, residuals_linear)

print(f"\nê°€ì„¤ ê²€ì •:")
print(f"   H0: Linear Regression = Baseline (ì°¨ì´ ì—†ìŒ)")
print(f"   H1: Linear Regression < Baseline (Linearê°€ ë” ì¢‹ìŒ)")

print(f"\nê²€ì • ê²°ê³¼:")
print(f"   - t-í†µê³„ëŸ‰: {t_stat:.3f}")
print(f"   - p-value:  {p_value:.6f}")
print(f"   - Cohen's d: {d:.3f}")

if p_value < 0.05:
    print(f"\nâœ… ê²°ë¡ : p < 0.05 â†’ H0 ê¸°ê°")
    print(f"   Linear Regressionì´ Baselineë³´ë‹¤ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ê²Œ ìš°ìˆ˜!")

    if abs(d) > 0.8:
        effect = "í°"
    elif abs(d) > 0.5:
        effect = "ì¤‘ê°„"
    elif abs(d) > 0.2:
        effect = "ì‘ì€"
    else:
        effect = "ë§¤ìš° ì‘ì€"

    print(f"   íš¨ê³¼ í¬ê¸°: {effect} íš¨ê³¼ (d={d:.3f})")
else:
    print(f"\nâŒ ê²°ë¡ : p â‰¥ 0.05 â†’ H0 ì±„íƒ")
    print(f"   Linear Regressionê³¼ Baseline ì°¨ì´ ì—†ìŒ")

print("\n" + "=" * 80)
print("âœ… ë² ì´ìŠ¤ë¼ì¸ & Linear Regression ëª¨ë¸ë§ ì™„ë£Œ!")
print("=" * 80)

print(f"""
ğŸ“Š ìš”ì•½:
   - Baseline MAE:         {mae_baseline:.4f}
   - Linear Regression MAE: {mae_linear:.4f}
   - ê°œì„ ìœ¨:                {improvement_mae:.1f}%
   - p-value:               {p_value:.6f}
   - í†µê³„ì  ìœ ì˜ì„±:          {"âœ… ìœ ì˜í•¨" if p_value < 0.05 else "âŒ ìœ ì˜í•˜ì§€ ì•ŠìŒ"}

ğŸ’¬ ë°œí‘œ í¬ì¸íŠ¸:
   ì •ê·œí™” ì—†ëŠ” ìˆœìˆ˜ ì„ í˜•íšŒê·€ë¡œ ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ 45% ì˜¤ì°¨ ê°ì†Œ ë‹¬ì„±
   í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ê°œì„  (p < 0.001, Cohen's d = ì¤‘ê°„ íš¨ê³¼)
""")

# ============================================================
# STEP 12: ê³„ìˆ˜ í•´ì„ (í”¼ì²˜ ì¤‘ìš”ë„)
# ============================================================

print("\n" + "=" * 80)
print("ğŸ“Š STEP 12: íšŒê·€ ê³„ìˆ˜ í•´ì„ (í”¼ì²˜ ì¤‘ìš”ë„)")
print("=" * 80)

# ê³„ìˆ˜ ë¶„ì„
coefficients = pd.DataFrame({
    'Feature': X_train.columns,
    'Coefficient': linear.coef_
}).sort_values('Coefficient', key=abs, ascending=False)

print("\n[12-1] ìƒìœ„ 10ê°œ ì¤‘ìš” ë³€ìˆ˜ (ì ˆëŒ“ê°’ ê¸°ì¤€):")
top10 = coefficients.head(10)
display(top10)

print("\n[12-2] í•˜ìœ„ 10ê°œ (ëœ ì¤‘ìš”):")
bottom10 = coefficients.tail(10)
display(bottom10)

# ê³„ìˆ˜ í•´ì„
print("\n[12-3] ê³„ìˆ˜ í•´ì„ ì˜ˆì‹œ:")
print("\nì–‘(+)ì˜ ê³„ìˆ˜ = í•´ë‹¹ ê°’ ì¦ê°€ ì‹œ ë‚™ì°°ê°€ìœ¨ ì¦ê°€")
positive = coefficients[coefficients['Coefficient'] > 0].head(5)
for idx, row in positive.iterrows():
    coef = row['Coefficient']
    feature = row['Feature']
    print(f"   {feature}: {coef:+.4f}")
    print(f"      â†’ 1 ë‹¨ìœ„ ì¦ê°€ ì‹œ ë‚™ì°°ê°€ìœ¨ {coef*100:+.2f}%p ë³€í™”")

print("\nìŒ(-)ì˜ ê³„ìˆ˜ = í•´ë‹¹ ê°’ ì¦ê°€ ì‹œ ë‚™ì°°ê°€ìœ¨ ê°ì†Œ")
negative = coefficients[coefficients['Coefficient'] < 0].head(5)
for idx, row in negative.iterrows():
    coef = row['Coefficient']
    feature = row['Feature']
    print(f"   {feature}: {coef:+.4f}")
    print(f"      â†’ 1 ë‹¨ìœ„ ì¦ê°€ ì‹œ ë‚™ì°°ê°€ìœ¨ {coef*100:+.2f}%p ë³€í™”")

print("\n" + "=" * 80)
print("âœ… ê³„ìˆ˜ í•´ì„ ì™„ë£Œ!")
print("=" * 80)

# ì‹œê°í™”
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 8))
top_features = coefficients.head(15)
colors = ['red' if x < 0 else 'green' for x in top_features['Coefficient']]
plt.barh(range(len(top_features)), top_features['Coefficient'], color=colors, alpha=0.7)
plt.yticks(range(len(top_features)), top_features['Feature'])
plt.xlabel('ê³„ìˆ˜ (Coefficient)', fontsize=12)
plt.title('Linear Regression ê³„ìˆ˜ (ìƒìœ„ 15ê°œ)', fontsize=14, fontweight='bold')
plt.axvline(0, color='black', linestyle='--', linewidth=1)
plt.grid(alpha=0.3, axis='x')
plt.tight_layout()
plt.show()

print("\nğŸ’¬ í•´ì„ ê°€ì´ë“œ:")
print("   - ë¹¨ê°„ìƒ‰ ë§‰ëŒ€: ë‚™ì°°ê°€ìœ¨ì„ ë‚®ì¶”ëŠ” ìš”ì¸")
print("   - ì´ˆë¡ìƒ‰ ë§‰ëŒ€: ë‚™ì°°ê°€ìœ¨ì„ ë†’ì´ëŠ” ìš”ì¸")
print("   - ë§‰ëŒ€ ê¸¸ì´: ì˜í–¥ë ¥ í¬ê¸°")