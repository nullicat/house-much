# ============================================================
# ğŸš€ TabNet - Tabular ë°ì´í„° ì „ìš© ë”¥ëŸ¬ë‹
# ============================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_absolute_error
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

print("=" * 80)
print("ğŸš€ TabNet - Tabular ì „ìš© ë”¥ëŸ¬ë‹")
print("=" * 80)

# ============================================================
# [0] ì„¤ì¹˜ í™•ì¸
# ============================================================

print("\n[0] TabNet ì„¤ì¹˜ í™•ì¸")

try:
    from pytorch_tabnet.tab_model import TabNetRegressor
    import torch
    print(f"   âœ… PyTorch: {torch.__version__}")
    print(f"   âœ… TabNet: Installed")
except ImportError:
    print("   âš ï¸ TabNet ì„¤ì¹˜ í•„ìš”")
    print("   ì„¤ì¹˜ ì¤‘...")
    import subprocess
    subprocess.check_call(['pip', 'install', 'pytorch-tabnet', '-q'])
    from pytorch_tabnet.tab_model import TabNetRegressor
    import torch
    print(f"   âœ… ì„¤ì¹˜ ì™„ë£Œ!")

# CUDA í™•ì¸
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"   Device: {device}")

# ============================================================
# [1] Checkpoint ë¡œë“œ
# ============================================================

print("\n[1] Checkpoint ë¡œë“œ")

import pickle

backup_dir = '/content/drive/MyDrive/auction_project_backup'

with open(f'{backup_dir}/checkpoint.pkl', 'rb') as f:
    checkpoint = pickle.load(f)

# ë°ì´í„° ì¶”ì¶œ
X_train = checkpoint['X_train']
X_test = checkpoint['X_test']
y_train = checkpoint['y_train']
y_test = checkpoint['y_test']

# Array ë³€í™˜
y_train_array = y_train.values if hasattr(y_train, 'values') else y_train
y_test_array = y_test.values if hasattr(y_test, 'values') else y_test
X_train_array = X_train.values if hasattr(X_train, 'values') else X_train
X_test_array = X_test.values if hasattr(X_test, 'values') else X_test

print(f"   Train: {X_train.shape}")
print(f"   Test: {X_test.shape}")

# ìŠ¤ì¼€ì¼ë§ (TabNetì€ ìŠ¤ì¼€ì¼ë§ ê¶Œì¥)
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_array)
X_test_scaled = scaler.transform(X_test_array)

print(f"   âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")

# ============================================================
# [2] TabNet ëª¨ë¸ êµ¬ì„±
# ============================================================

print("\n[2] TabNet ëª¨ë¸ êµ¬ì„±")

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
tabnet_params = {
    'n_d': 64,  # Feature dimension for decision
    'n_a': 64,  # Feature dimension for attention
    'n_steps': 5,  # Number of decision steps
    'gamma': 1.5,  # Coefficient for feature reusage
    'n_independent': 2,  # Number of independent GLU layers
    'n_shared': 2,  # Number of shared GLU layers
    'lambda_sparse': 0.0001,  # Sparsity regularization
    'momentum': 0.3,  # Momentum for batch normalization
    'clip_value': 2.0,  # Gradient clipping
    'optimizer_fn': torch.optim.Adam,
    'optimizer_params': dict(lr=0.02),
    'scheduler_fn': torch.optim.lr_scheduler.StepLR,
    'scheduler_params': {"step_size": 50, "gamma": 0.9},
    'mask_type': 'entmax',  # 'sparsemax' or 'entmax'
    'seed': 42,
    'verbose': 10,
    'device_name': device
}

print(f"   n_d (Feature dim): {tabnet_params['n_d']}")
print(f"   n_a (Attention dim): {tabnet_params['n_a']}")
print(f"   n_steps (Decision steps): {tabnet_params['n_steps']}")
print(f"   gamma (Feature reuse): {tabnet_params['gamma']}")

# ëª¨ë¸ ìƒì„±
tabnet = TabNetRegressor(**tabnet_params)

print(f"   âœ… TabNet ëª¨ë¸ ìƒì„± ì™„ë£Œ")

# ============================================================
# [3] í•™ìŠµ
# ============================================================

print("\n[3] í•™ìŠµ ì‹œì‘ (ì•½ 5~10ë¶„ ì†Œìš”)")

# Train/Validation ë¶„ë¦¬
from sklearn.model_selection import train_test_split

X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
    X_train_scaled, y_train_array,
    test_size=0.2,
    random_state=42
)

print(f"   Train: {X_train_split.shape}")
print(f"   Val: {X_val_split.shape}")

# í•™ìŠµ
tabnet.fit(
    X_train=X_train_split,
    y_train=y_train_split.reshape(-1, 1),
    eval_set=[(X_val_split, y_val_split.reshape(-1, 1))],
    eval_name=['val'],
    eval_metric=['mae'],
    max_epochs=200,
    patience=20,
    batch_size=256,
    virtual_batch_size=128,
    num_workers=0,
    drop_last=False,
    loss_fn=torch.nn.HuberLoss(delta=1.35)  # Huber Loss ì‚¬ìš©!
)

print(f"\n   âœ… í•™ìŠµ ì™„ë£Œ")

# ============================================================
# [4] í‰ê°€
# ============================================================

print("\n" + "=" * 80)
print("[4] í‰ê°€")
print("=" * 80)

# ì˜ˆì¸¡
pred_tabnet = tabnet.predict(X_test_scaled).flatten()

# ì „ì²´ ì„±ëŠ¥
mae_total = mean_absolute_error(y_test_array, pred_tabnet)
print(f"\n   ì „ì²´ MAE: {mae_total:.4f}")

baseline_mae = 0.0715
improvement = (baseline_mae - mae_total) / baseline_mae * 100
print(f"   Baseline: {baseline_mae:.4f}")
print(f"   ê°œì„ : {improvement:+.1f}%")

# ì €ê°€ êµ¬ê°„
low_mask = (y_test_array < 0.5)
mae_low = mean_absolute_error(y_test_array[low_mask], pred_tabnet[low_mask])
baseline_low = 0.0805

print(f"\n   ì €ê°€ MAE: {mae_low:.4f}")
print(f"   Baseline: {baseline_low:.4f}")
print(f"   ê°œì„ : {(baseline_low - mae_low) / baseline_low * 100:+.1f}%")

# Within 5%p
abs_errors = np.abs(pred_tabnet - y_test_array)
within_5p_total = (abs_errors <= 0.05).sum() / len(abs_errors) * 100
within_5p_low = (abs_errors[low_mask] <= 0.05).sum() / low_mask.sum() * 100

print(f"\n   ì „ì²´ Within 5%p: {within_5p_total:.1f}%")
print(f"   Baseline: 49.7%")
print(f"   ê°œì„ : {within_5p_total - 49.7:+.1f}%p")

print(f"\n   ì €ê°€ Within 5%p: {within_5p_low:.1f}%")
print(f"   Baseline: 25.7%")
print(f"   ê°œì„ : {within_5p_low - 25.7:+.1f}%p")

# Within 10%p
within_10p_total = (abs_errors <= 0.10).sum() / len(abs_errors) * 100
print(f"\n   ì „ì²´ Within 10%p: {within_10p_total:.1f}%")
print(f"   Baseline: 85.7%")

# ============================================================
# [5] Feature Importance (TabNetì˜ ê°•ì !)
# ============================================================

print("\n" + "=" * 80)
print("[5] Feature Importance")
print("=" * 80)

# Feature Importance ì¶”ì¶œ
importance = tabnet.feature_importances_

# ì •ë ¬
feature_importance = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': importance
}).sort_values('Importance', ascending=False)

print("\n[Top 10 Features]")
print(feature_importance.head(10).to_string(index=False))

# ============================================================
# [6] í˜¼ë™í–‰ë ¬
# ============================================================

print("\n" + "=" * 80)
print("[6] í˜¼ë™í–‰ë ¬")
print("=" * 80)

def categorize_error(error):
    if error <= 0.03:
        return 'Excellent'
    elif error <= 0.05:
        return 'Good'
    elif error <= 0.10:
        return 'Fair'
    else:
        return 'Poor'

tabnet_categories = [categorize_error(e) for e in abs_errors]
tabnet_counts = Counter(tabnet_categories)

total = len(abs_errors)

print("\n[TabNet]")
print(f"   Excellent (â‰¤3%p): {tabnet_counts['Excellent']/total*100:.1f}%")
print(f"   Good (3~5%p):     {tabnet_counts['Good']/total*100:.1f}%")
print(f"   Fair (5~10%p):    {tabnet_counts['Fair']/total*100:.1f}%")
print(f"   Poor (>10%p):     {tabnet_counts['Poor']/total*100:.1f}%")

# Baseline ë¹„êµ
if 'final_ensemble_pred' in checkpoint or 'ensemble_pred' in checkpoint:
    baseline_pred = checkpoint.get('final_ensemble_pred') or checkpoint.get('ensemble_pred')
    if baseline_pred is not None:
        errors_baseline = np.abs(baseline_pred - y_test_array)
        baseline_categories = [categorize_error(e) for e in errors_baseline]
        baseline_counts = Counter(baseline_categories)

        print("\n[Baseline (Ensemble)]")
        print(f"   Excellent (â‰¤3%p): {baseline_counts['Excellent']/total*100:.1f}%")
        print(f"   Good (3~5%p):     {baseline_counts['Good']/total*100:.1f}%")
        print(f"   Fair (5~10%p):    {baseline_counts['Fair']/total*100:.1f}%")
        print(f"   Poor (>10%p):     {baseline_counts['Poor']/total*100:.1f}%")

        print("\n[ê°œì„ ìœ¨]")
        improvements = {
            'Excellent': (tabnet_counts['Excellent'] - baseline_counts['Excellent'])/total*100,
            'Good': (tabnet_counts['Good'] - baseline_counts['Good'])/total*100,
            'Fair': (tabnet_counts['Fair'] - baseline_counts['Fair'])/total*100,
            'Poor': (tabnet_counts['Poor'] - baseline_counts['Poor'])/total*100
        }

        for cat, imp in improvements.items():
            symbol = "âœ…" if (imp > 0 and cat in ['Excellent', 'Good']) or (imp < 0 and cat in ['Fair', 'Poor']) else "âš ï¸"
            print(f"   {cat:10s} {imp:+.1f}%p {symbol}")

# ============================================================
# [7] ì‹œê°í™”
# ============================================================

print("\n[7] ì‹œê°í™”")

fig = plt.figure(figsize=(20, 12))

# 1. í•™ìŠµ ê³¡ì„ 
ax1 = plt.subplot(3, 3, 1)

# TabNetì˜ í•™ìŠµ ì´ë ¥ ì¶”ì¶œ
history_df = pd.DataFrame(tabnet.history)

if 'loss' in history_df.columns:
    ax1.plot(history_df['loss'], label='Train Loss', linewidth=2)
    ax1.plot(history_df['val_0_mae'], label='Val MAE', linewidth=2)
    ax1.set_xlabel('Epoch', fontsize=10, fontweight='bold')
    ax1.set_ylabel('Loss / MAE', fontsize=10, fontweight='bold')
    ax1.set_title('í•™ìŠµ ê³¡ì„ ', fontsize=11, fontweight='bold')
    ax1.legend()
    ax1.grid(alpha=0.3)

# 2. Feature Importance
ax2 = plt.subplot(3, 3, 2)
top_features = feature_importance.head(15)
ax2.barh(range(len(top_features)), top_features['Importance'].values,
         color='steelblue', edgecolor='black', alpha=0.8)
ax2.set_yticks(range(len(top_features)))
ax2.set_yticklabels(top_features['Feature'].values, fontsize=8)
ax2.set_xlabel('Importance', fontsize=10, fontweight='bold')
ax2.set_title('Feature Importance (Top 15)', fontsize=11, fontweight='bold')
ax2.grid(alpha=0.3, axis='x')

# 3. í˜¼ë™í–‰ë ¬ (TabNet)
ax3 = plt.subplot(3, 3, 4)
categories = ['Excellent', 'Good', 'Fair', 'Poor']
tabnet_pcts = [tabnet_counts[c]/total*100 for c in categories]

bars = ax3.bar(range(len(categories)), tabnet_pcts,
               color=['green', 'lightgreen', 'orange', 'red'],
               edgecolor='black', linewidth=2, alpha=0.8)

ax3.set_xticks(range(len(categories)))
ax3.set_xticklabels(categories, fontsize=9, fontweight='bold')
ax3.set_ylabel('Percentage (%)', fontsize=10, fontweight='bold')
ax3.set_title(f'TabNet\nWithin 5%p: {within_5p_total:.1f}%',
              fontsize=11, fontweight='bold')
ax3.grid(alpha=0.3, axis='y')
ax3.set_ylim(0, 50)

for bar, pct in zip(bars, tabnet_pcts):
    ax3.text(bar.get_x() + bar.get_width()/2, pct + 1,
             f'{pct:.1f}%', ha='center', fontsize=8, fontweight='bold')

# 4. ì˜¤ì°¨ ë¶„í¬
ax4 = plt.subplot(3, 3, 5)
ax4.hist(abs_errors, bins=50, alpha=0.7, color='steelblue', edgecolor='black')
ax4.axvline(0.05, color='green', linestyle='--', linewidth=2, label='5%p')
ax4.axvline(0.10, color='orange', linestyle='--', linewidth=2, label='10%p')
ax4.set_xlabel('Absolute Error', fontsize=10, fontweight='bold')
ax4.set_ylabel('Frequency', fontsize=10, fontweight='bold')
ax4.set_title('ì˜¤ì°¨ ë¶„í¬', fontsize=11, fontweight='bold')
ax4.legend()
ax4.grid(alpha=0.3)

# 5. ì˜ˆì¸¡ vs ì‹¤ì œ (ì „ì²´)
ax5 = plt.subplot(3, 3, 6)
ax5.scatter(y_test_array, pred_tabnet, alpha=0.3, s=10)
ax5.plot([0, 2], [0, 2], 'r--', linewidth=2, label='Perfect')
ax5.set_xlabel('ì‹¤ì œê°’', fontsize=10, fontweight='bold')
ax5.set_ylabel('ì˜ˆì¸¡ê°’', fontsize=10, fontweight='bold')
ax5.set_title(f'ì˜ˆì¸¡ vs ì‹¤ì œ (ì „ì²´)\nMAE: {mae_total:.4f}',
              fontsize=11, fontweight='bold')
ax5.legend()
ax5.grid(alpha=0.3)

# 6. ì˜ˆì¸¡ vs ì‹¤ì œ (ì €ê°€)
ax6 = plt.subplot(3, 3, 7)
ax6.scatter(y_test_array[low_mask], pred_tabnet[low_mask],
            alpha=0.5, s=20, color='red')
ax6.plot([0, 0.5], [0, 0.5], 'r--', linewidth=2, label='Perfect')
ax6.set_xlabel('ì‹¤ì œê°’', fontsize=10, fontweight='bold')
ax6.set_ylabel('ì˜ˆì¸¡ê°’', fontsize=10, fontweight='bold')
ax6.set_title(f'ì˜ˆì¸¡ vs ì‹¤ì œ (ì €ê°€)\nMAE: {mae_low:.4f}',
              fontsize=11, fontweight='bold')
ax6.legend()
ax6.grid(alpha=0.3)

# 7. ì˜¤ì°¨ ëˆ„ì  ë¶„í¬ (CDF)
ax7 = plt.subplot(3, 3, 8)
sorted_errors = np.sort(abs_errors)
cdf = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors) * 100

ax7.plot(sorted_errors, cdf, linewidth=2)
ax7.axvline(0.05, color='green', linestyle='--', linewidth=2, label='5%p')
ax7.axvline(0.10, color='orange', linestyle='--', linewidth=2, label='10%p')
ax7.axhline(50, color='gray', linestyle=':', alpha=0.5)
ax7.set_xlabel('Absolute Error', fontsize=10, fontweight='bold')
ax7.set_ylabel('Cumulative %', fontsize=10, fontweight='bold')
ax7.set_title('ëˆ„ì  ì˜¤ì°¨ ë¶„í¬ (CDF)', fontsize=11, fontweight='bold')
ax7.legend()
ax7.grid(alpha=0.3)

# 8. êµ¬ê°„ë³„ ì„±ëŠ¥
ax8 = plt.subplot(3, 3, 9)
bins = [0, 0.01, 0.03, 0.05, 0.07, 0.10, 0.15, 0.20, 1.0]
labels = ['0-1%', '1-3%', '3-5%', '5-7%', '7-10%', '10-15%', '15-20%', '>20%']
error_dist = pd.cut(abs_errors, bins=bins, labels=labels).value_counts().sort_index()

bars = ax8.bar(range(len(error_dist)), error_dist.values / len(abs_errors) * 100,
               color=['darkgreen', 'green', 'lightgreen', 'yellow',
                      'orange', 'darkorange', 'red', 'darkred'],
               edgecolor='black', alpha=0.8)

ax8.set_xticks(range(len(error_dist)))
ax8.set_xticklabels(labels, rotation=45, fontsize=8)
ax8.set_ylabel('Percentage (%)', fontsize=10, fontweight='bold')
ax8.set_title('êµ¬ê°„ë³„ ì˜¤ì°¨ ë¶„í¬', fontsize=11, fontweight='bold')
ax8.grid(alpha=0.3, axis='y')

for bar, val in zip(bars, error_dist.values / len(abs_errors) * 100):
    ax8.text(bar.get_x() + bar.get_width()/2, val + 0.5,
             f'{val:.1f}%', ha='center', fontsize=7)

plt.tight_layout()
plt.savefig(f'{backup_dir}/tabnet_results.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"   âœ… ì €ì¥: tabnet_results.png")

# ============================================================
# [8] ìµœì¢… ìš”ì•½
# ============================================================

print("\n" + "=" * 80)
print("ğŸ† ìµœì¢… ìš”ì•½")
print("=" * 80)

print(f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                TabNet ì„±ëŠ¥ ë¹„êµ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

[ì „ì²´ ì„±ëŠ¥]
Baseline MAE:       {baseline_mae:.4f}
TabNet MAE:         {mae_total:.4f}
ê°œì„ :               {improvement:+.1f}%

[ì €ê°€ ì„±ëŠ¥]
Baseline MAE:       {baseline_low:.4f}
TabNet MAE:         {mae_low:.4f}
ê°œì„ :               {(baseline_low - mae_low) / baseline_low * 100:+.1f}%

[Within 5%p]
Baseline (ì „ì²´):    49.7%
TabNet (ì „ì²´):      {within_5p_total:.1f}% ({within_5p_total - 49.7:+.1f}%p)

Baseline (ì €ê°€):    25.7%
TabNet (ì €ê°€):      {within_5p_low:.1f}% ({within_5p_low - 25.7:+.1f}%p)

[í˜¼ë™í–‰ë ¬]
Excellent + Good:   {(tabnet_counts['Excellent'] + tabnet_counts['Good'])/total*100:.1f}%
Fair:               {tabnet_counts['Fair']/total*100:.1f}%
Poor:               {tabnet_counts['Poor']/total*100:.1f}%

[Within 10%p]
TabNet:             {within_10p_total:.1f}%
Baseline:           85.7%

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
""")

# í‰ê°€
if improvement > 5:
    print("ğŸ‰ğŸ‰ğŸ‰ ëŒ€í­ ê°œì„ ! TabNet íš¨ê³¼ í™•ì¸!")
elif improvement > 2:
    print("âœ…âœ… ê°œì„ ë¨! TabNet íš¨ê³¼ ìˆìŒ!")
elif improvement > 0:
    print("âœ… ì†Œí­ ê°œì„ ")
else:
    print("âš ï¸ ê°œì„  ì•ˆ ë¨. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í•„ìš”")

if within_5p_low > 35:
    print(f"ğŸ‰ğŸ‰ğŸ‰ ì €ê°€ êµ¬ê°„ ëŒ€í­ ê°œì„ ! (+{within_5p_low - 25.7:.1f}%p)")
elif within_5p_low > 30:
    print(f"âœ…âœ… ì €ê°€ êµ¬ê°„ ê°œì„ ! (+{within_5p_low - 25.7:.1f}%p)")

if tabnet_counts['Poor']/total*100 < 12:
    print("ğŸ‰ğŸ‰ğŸ‰ Poor êµ¬ê°„ í¬ê²Œ ê°ì†Œ! ëª©í‘œ ë‹¬ì„±!")
elif tabnet_counts['Poor']/total*100 < 14:
    print("âœ…âœ… Poor êµ¬ê°„ ê°ì†Œ!")

print("\n" + "=" * 80)

# ============================================================
# [9] ê²°ê³¼ ì €ì¥
# ============================================================

print("\n[9] ê²°ê³¼ ì €ì¥")

# TabNet ëª¨ë¸ ì €ì¥
tabnet.save_model(f'{backup_dir}/tabnet_model')
print(f"   âœ… TabNet ëª¨ë¸ ì €ì¥: tabnet_model.zip")

# ì˜ˆì¸¡ê°’ ì €ì¥
tabnet_predictions = {
    'y_test': y_test_array,
    'pred': pred_tabnet,
    'mae_total': mae_total,
    'mae_low': mae_low,
    'within_5p_total': within_5p_total,
    'within_5p_low': within_5p_low,
    'within_10p_total': within_10p_total,
    'tabnet_counts': tabnet_counts,
    'feature_importance': feature_importance
}

# Checkpointì— ì¶”ê°€
checkpoint['tabnet_predictions'] = tabnet_predictions

# ì €ì¥
with open(f'{backup_dir}/checkpoint.pkl', 'wb') as f:
    pickle.dump(checkpoint, f)

print(f"   âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: checkpoint.pklì— ì¶”ê°€")
print("\n" + "=" * 80)