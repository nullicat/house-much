# ============================================================
# ğŸ” ML ëª¨ë¸ ì¬ê²€ì¦ (ì •ê·œí™” í¬í•¨)
# ============================================================

print("=" * 80)
print("ğŸ” ML ëª¨ë¸ ì¬ê²€ì¦ (ì •ê·œí™”)")
print("=" * 80)

print("""
ì˜ì‹¬:
checkpointì˜ ë‹¤ë¥¸ ML ëª¨ë¸ë“¤ë„
ì •ê·œí™” ì•ˆ í•´ì„œ ì„±ëŠ¥ ë‚®ì•˜ì„ ê°€ëŠ¥ì„±!

ì¬ê²€ì¦:
- Linear, Ridge, Lasso
- Random Forest
- XGBoost, LightGBM
- CatBoost

ëª¨ë‘ ì •ê·œí™”í•´ì„œ ì¬í•™ìŠµ!
""")

from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

# ============================================================
# [1] ì •ê·œí™”ëœ ë°ì´í„° (ì´ë¯¸ ìˆìŒ)
# ============================================================

print("\n[1] ë°ì´í„° ì¤€ë¹„")
print(f"   ì •ê·œí™”ëœ Train: {X_train_scaled.shape}")
print(f"   ì •ê·œí™”ëœ Test: {X_test_scaled.shape}")

# ============================================================
# [2] ëª¨ë¸ ì¬í•™ìŠµ
# ============================================================

print("\n[2] ëª¨ë¸ ì¬í•™ìŠµ (ì •ê·œí™” ë°ì´í„°)")
print("   (ì•½ 2~3ë¶„ ì†Œìš”)")

models_to_test = {
    'Linear Regression': LinearRegression(),
    'Ridge': Ridge(alpha=1.0),
    'Lasso': Lasso(alpha=0.01),
    'Huber': HuberRegressor(epsilon=1.35, alpha=0.0001),
    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),
    'XGBoost': XGBRegressor(n_estimators=100, max_depth=8, random_state=42, verbosity=0),
    'LightGBM': LGBMRegressor(n_estimators=100, max_depth=8, random_state=42, verbose=-1),
}

ml_results = []

print("\nê²°ê³¼:")
print("-" * 70)

for name, model in models_to_test.items():
    # í•™ìŠµ
    model.fit(X_train_scaled, y_train_array)

    # ì˜ˆì¸¡
    pred = model.predict(X_test_scaled)

    # í‰ê°€
    mae = mean_absolute_error(y_test_array, pred)
    rmse = np.sqrt(mean_squared_error(y_test_array, pred))
    r2 = r2_score(y_test_array, pred)

    ml_results.append({
        'Model': name,
        'MAE': mae,
        'RMSE': rmse,
        'RÂ²': r2,
        'Data': '24ê°œ (ì •ê·œí™”)'
    })

    print(f"   {name:20s} MAE: {mae:.4f}  RMSE: {rmse:.4f}  RÂ²: {r2:.4f}")

# CatBoost (10ê°œ í”¼ì²˜, ì •ê·œí™” ì•ˆ í•¨)
ml_results.append({
    'Model': 'CatBoost',
    'MAE': catboost_test_mae,
    'RMSE': catboost_test_rmse,
    'RÂ²': catboost_test_r2,
    'Data': '10ê°œ (ì„ íƒ)'
})

# Ensemble
ml_results.append({
    'Model': 'Ensemble (Huber+CatBoost)',
    'MAE': best_ensemble_mae,
    'RMSE': np.sqrt(mean_squared_error(y_test_array, final_ensemble_pred)),
    'RÂ²': final_ensemble_r2,
    'Data': '24ê°œ + 10ê°œ'
})

# ============================================================
# [3] ë¹„êµ ë¶„ì„
# ============================================================

print("\n[3] ì „ì²´ ë¹„êµ")

ml_results_df = pd.DataFrame(ml_results).sort_values('MAE')

print("\nğŸ“Š ML ëª¨ë¸ ìˆœìœ„ (ì •ê·œí™” í¬í•¨):")
print(ml_results_df.to_string(index=False))

# ìµœê³  ëª¨ë¸
best_ml = ml_results_df.iloc[0]

print(f"\nğŸ† ìµœê³  ML ëª¨ë¸:")
print(f"   ëª¨ë¸: {best_ml['Model']}")
print(f"   MAE: {best_ml['MAE']:.4f}")
print(f"   RÂ²: {best_ml['RÂ²']:.4f}")

# ============================================================
# [4] ì¶©ê²© ë¶„ì„
# ============================================================

print("\n[4] ì¶©ê²©ì  ë°œê²¬?")

# Linear ê³„ì—´ í™•ì¸
linear_models = ml_results_df[ml_results_df['Model'].isin(['Linear Regression', 'Ridge', 'Lasso', 'Huber'])]

print(f"\nLinear ê³„ì—´ ì„±ëŠ¥:")
print(linear_models[['Model', 'MAE', 'RÂ²']].to_string(index=False))

best_linear = linear_models.iloc[0]

if best_linear['MAE'] < 0.0753:
    print(f"\n   ğŸ’¥ ì¶©ê²©! {best_linear['Model']}ì´ CatBoost(0.0753)ë³´ë‹¤ ì¢‹ìŒ!")
    print(f"   â†’ ë‹¹ì‹œ ì •ê·œí™” ì•ˆ í•´ì„œ ë†“ì³¤ì„ ê°€ëŠ¥ì„±!")
else:
    print(f"\n   â†’ CatBoostê°€ ì—¬ì „íˆ ìš°ìˆ˜")

# ============================================================
# [5] ì‹œê°í™”
# ============================================================

print("\n[5] ì‹œê°í™”")

fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# 1. MAE ë¹„êµ
ax1 = axes[0]

models = ml_results_df['Model']
maes = ml_results_df['MAE']

bars = ax1.barh(range(len(models)), maes)

# ìƒ‰ìƒ (ë‚®ì„ìˆ˜ë¡ ë…¹ìƒ‰)
colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.9, len(models)))
for bar, color in zip(bars, colors):
    bar.set_color(color)
    bar.set_edgecolor('black')
    bar.set_linewidth(1.5)

ax1.set_yticks(range(len(models)))
ax1.set_yticklabels(models, fontsize=10)
ax1.set_xlabel('MAE', fontsize=12, fontweight='bold')
ax1.set_title('ML ëª¨ë¸ ë¹„êµ (ì •ê·œí™” í¬í•¨)', fontsize=13, fontweight='bold')
ax1.invert_yaxis()

# ê°’ í‘œì‹œ
for i, mae in enumerate(maes):
    ax1.text(mae + 0.002, i, f"{mae:.4f}", va='center', fontsize=9)

# Baseline
ax1.axvline(x=0.1402, color='red', linestyle='--', linewidth=2,
            label='Baseline', alpha=0.7)
ax1.legend()
ax1.grid(axis='x', alpha=0.3)

# 2. RÂ² ë¹„êµ
ax2 = axes[1]

r2_values = ml_results_df['RÂ²']
bars = ax2.barh(range(len(models)), r2_values)

for bar, color in zip(bars, colors):
    bar.set_color(color)
    bar.set_edgecolor('black')
    bar.set_linewidth(1.5)

ax2.set_yticks(range(len(models)))
ax2.set_yticklabels(models, fontsize=10)
ax2.set_xlabel('RÂ² Score', fontsize=12, fontweight='bold')
ax2.set_title('RÂ² ë¹„êµ', fontsize=13, fontweight='bold')
ax2.invert_yaxis()

for i, r2 in enumerate(r2_values):
    ax2.text(r2 + 0.01, i, f"{r2:.4f}", va='center', fontsize=9)

ax2.grid(axis='x', alpha=0.3)

plt.tight_layout()
plt.savefig(f'{backup_dir}/ml_models_revalidation.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"\nâœ… ì €ì¥: ml_models_revalidation.png")

# ============================================================
# ê²°ë¡ 
# ============================================================

print("\n" + "=" * 80)
print("ğŸ¯ ê²°ë¡ ")
print("=" * 80)

if best_ml['Model'] in ['Linear Regression', 'Ridge', 'Lasso', 'Huber']:
    if best_ml['MAE'] < 0.0753:
        print(f"""
ğŸ’¥ ì¶©ê²©ì  ë°œê²¬!

{best_ml['Model']}ì´ CatBoostë³´ë‹¤ ìš°ìˆ˜!
- {best_ml['Model']}: {best_ml['MAE']:.4f}
- CatBoost: 0.0753

ì›ì¸:
â†’ ë‹¹ì‹œ Linear ëª¨ë¸ë“¤ì„ ì •ê·œí™” ì•ˆ í•¨
â†’ ì„±ëŠ¥ì´ ë‚®ê²Œ ì¸¡ì •ë¨
â†’ CatBoostë§Œ ì„ íƒ

êµí›ˆ:
ë°ì´í„° ì „ì²˜ë¦¬ê°€ ëª¨ë¸ ì„ íƒì— ê²°ì •ì !
        """)
    else:
        print(f"""
ì˜ˆìƒëŒ€ë¡œ!

Linear ëª¨ë¸ë“¤ë„ ì •ê·œí™”í•˜ë©´ ì¢‹ì•„ì§€ì§€ë§Œ,
ì—¬ì „íˆ CatBoost/Ensembleì´ ìµœê³ !

- {best_ml['Model']}: {best_ml['MAE']:.4f}
- CatBoost: {catboost_test_mae:.4f}
- Ensemble: {best_ensemble_mae:.4f}

â†’ ì´ˆê¸° ë¶„ì„ì´ ì˜³ì•˜ìŒ!
        """)
else:
    print(f"""
ì˜ˆìƒ ì™¸!

{best_ml['Model']}ì´ ìµœê³ !

ì¬í‰ê°€ í•„ìš”í•  ìˆ˜ë„...
    """)

print("âœ… ML ëª¨ë¸ ì¬ê²€ì¦ ì™„ë£Œ!")