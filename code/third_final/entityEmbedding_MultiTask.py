# ============================================================
# ğŸš€ Entity Embedding + Multi-Task Learning (êµ¬/ë™ ìˆ˜ì • ë²„ì „)
# ============================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_absolute_error, accuracy_score, f1_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# TensorFlow ì„í¬íŠ¸
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

print("=" * 80)
print("ğŸš€ Entity Embedding + Multi-Task ë”¥ëŸ¬ë‹ (êµ¬/ë™ ìˆ˜ì • ë²„ì „)")
print("=" * 80)
print(f"TensorFlow ë²„ì „: {tf.__version__}")

# ============================================================
# [0] Checkpoint ë¡œë“œ + êµ¬/ë™ ì¸ì½”ë”© ìˆ˜ì •
# ============================================================

print("\n[0] Checkpoint ë¡œë“œ + êµ¬/ë™ ì¸ì½”ë”©")

import pickle

backup_dir = '/content/drive/MyDrive/auction_project_backup'

with open(f'{backup_dir}/checkpoint.pkl', 'rb') as f:
    checkpoint = pickle.load(f)

# df_soldì—ì„œ ì›ë³¸ êµ¬/ë™ ì¸ì½”ë”©
df_sold = checkpoint['df_sold']

print(f"   df_sold: {df_sold.shape}")

# êµ¬/ë™ ì¸ì½”ë”© (ì›ë³¸ì—ì„œ)
le_gu = LabelEncoder()
le_dong = LabelEncoder()

df_sold['êµ¬_encoded_new'] = le_gu.fit_transform(df_sold['êµ¬'])
df_sold['ë™_encoded_new'] = le_dong.fit_transform(df_sold['ë™'])

print(f"   âœ… ì¸ì½”ë”© ì™„ë£Œ")
print(f"   êµ¬: {len(le_gu.classes_)}ê°œ")
print(f"   ë™: {len(le_dong.classes_)}ê°œ")

# Train/Test ë°ì´í„°
X_train = checkpoint['X_train']
X_test = checkpoint['X_test']
y_train = checkpoint['y_train']
y_test = checkpoint['y_test']

# Train/Test ì¸ë±ìŠ¤ ì¶”ì¶œ
train_indices = X_train.index
test_indices = X_test.index

# êµ¬/ë™ ì¸ì½”ë”© ì¶”ì¶œ
gu_train = df_sold.loc[train_indices, 'êµ¬_encoded_new'].values
gu_test = df_sold.loc[test_indices, 'êµ¬_encoded_new'].values

dong_train = df_sold.loc[train_indices, 'ë™_encoded_new'].values
dong_test = df_sold.loc[test_indices, 'ë™_encoded_new'].values

print(f"\n   êµ¬ ë²”ìœ„ (train): {gu_train.min()} ~ {gu_train.max()}")
print(f"   ë™ ë²”ìœ„ (train): {dong_train.min()} ~ {dong_train.max()}")

print(f"\n   êµ¬ ìƒ˜í”Œ: {gu_train[:5]}")
print(f"   ë™ ìƒ˜í”Œ: {dong_train[:5]}")

# ìˆ˜ì¹˜í˜• ë°ì´í„° ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"   âœ… ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ")

# Array ë³€í™˜
y_train_array = y_train.values if hasattr(y_train, 'values') else y_train
y_test_array = y_test.values if hasattr(y_test, 'values') else y_test

print(f"   âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")

# ============================================================
# [1] ë°ì´í„° ì¤€ë¹„
# ============================================================

print("\n[1] ë°ì´í„° ì¤€ë¹„")

print(f"   Train: {X_train.shape}")
print(f"   Test: {X_test.shape}")

# ìˆ˜ì¹˜í˜• í”¼ì²˜ (15ê°œ)
numeric_features = ['ì¸µ', 'í† ì§€ë©´ì ', 'ê±´ë¬¼ë©´ì ', 'ê°ì •ê°€', 'ìµœì €ê°€',
                    'ìœ ì°°íšŸìˆ˜', 'ìµœì €ê°€ìœ¨', 'ë³´ì¦ê¸ˆë¹„ìœ¨', 'í† ì§€ê±´ë¬¼ë¹„ìœ¨',
                    'í‰ë‹¹ê°ì •ê°€', 'ë³´ì¦ê¸ˆìœ ë¬´', 'ì„ ìˆœìœ„ì´ˆê³¼', 'ì‹ ê±´ì—¬ë¶€',
                    'ë§¤ê°_ì›”', 'ë§¤ê°_ë¶„ê¸°']

# ì¸ë±ìŠ¤
numeric_indices = [list(X_train.columns).index(f) for f in numeric_features]

# ìˆ˜ì¹˜í˜• ë°ì´í„°
X_train_numeric = X_train_scaled[:, numeric_indices]
X_test_numeric = X_test_scaled[:, numeric_indices]

print(f"   ìˆ˜ì¹˜í˜•: {X_train_numeric.shape}")

# ë²”ì£¼í˜• ë°ì´í„° (êµ¬/ë™ - ì •ìˆ˜!)
X_train_gu = gu_train.reshape(-1, 1)
X_train_dong = dong_train.reshape(-1, 1)

X_test_gu = gu_test.reshape(-1, 1)
X_test_dong = dong_test.reshape(-1, 1)

print(f"   ë²”ì£¼í˜• (êµ¬): {X_train_gu.shape}")
print(f"   ë²”ì£¼í˜• (ë™): {X_train_dong.shape}")

# ë¶„ë¥˜ íƒ€ê²Ÿ
y_train_class = (y_train_array < 0.5).astype(int)
y_test_class = (y_test_array < 0.5).astype(int)

print(f"   ìœ ì°° ì¼€ì´ìŠ¤: {y_train_class.sum()}ê°œ ({y_train_class.sum()/len(y_train_class)*100:.1f}%)")

# ============================================================
# [2] ëª¨ë¸ êµ¬ì¡°
# ============================================================

print("\n[2] ëª¨ë¸ êµ¬ì¡° ì •ì˜")

# êµ¬, ë™ì˜ ê³ ìœ ê°’ ê°œìˆ˜
n_gu = len(le_gu.classes_)
n_dong = len(le_dong.classes_)

print(f"   êµ¬: {n_gu}ê°œ")
print(f"   ë™: {n_dong}ê°œ")

# ì„ë² ë”© ì°¨ì›
embedding_dim_gu = 5
embedding_dim_dong = 10  # ë™ì´ ë” ë§ìœ¼ë‹ˆ ì°¨ì› ë†’ì„

# Input Layers
input_numeric = layers.Input(shape=(len(numeric_features),), name='numeric_input')
input_gu = layers.Input(shape=(1,), name='gu_input')
input_dong = layers.Input(shape=(1,), name='dong_input')

# Embedding Layers
embedding_gu = layers.Embedding(
    input_dim=n_gu,
    output_dim=embedding_dim_gu,
    name='gu_embedding'
)(input_gu)
embedding_gu = layers.Flatten()(embedding_gu)

embedding_dong = layers.Embedding(
    input_dim=n_dong,
    output_dim=embedding_dim_dong,
    name='dong_embedding'
)(input_dong)
embedding_dong = layers.Flatten()(embedding_dong)

# Concatenate
concat = layers.Concatenate()([
    input_numeric,
    embedding_gu,
    embedding_dong
])

# Shared Layers (Body)
x = layers.Dense(256, activation='relu')(concat)
x = layers.BatchNormalization()(x)
x = layers.Dropout(0.3)(x)

x = layers.Dense(128, activation='relu')(x)
x = layers.BatchNormalization()(x)
x = layers.Dropout(0.2)(x)

x = layers.Dense(64, activation='relu')(x)
x = layers.Dropout(0.2)(x)

# Multi-Task Heads
# Head 1: ë¶„ë¥˜ (ìœ ì°° ì—¬ë¶€)
classification_head = layers.Dense(32, activation='relu', name='class_dense')(x)
classification_output = layers.Dense(1, activation='sigmoid', name='classification')(classification_head)

# Head 2: íšŒê·€ (ë‚™ì°°ê°€ìœ¨)
regression_head = layers.Dense(32, activation='relu', name='reg_dense')(x)
regression_output = layers.Dense(1, name='regression')(regression_head)

# Model
model = keras.Model(
    inputs=[input_numeric, input_gu, input_dong],
    outputs=[classification_output, regression_output]
)

print("   âœ… ëª¨ë¸ êµ¬ì¡° ì™„ì„±")
print(f"   íŒŒë¼ë¯¸í„°: {model.count_params():,}ê°œ")

# ============================================================
# [3] ì»´íŒŒì¼
# ============================================================

print("\n[3] ì»´íŒŒì¼")

# Multi-task Loss (íšŒê·€ ì¤‘ì‹œ)
model.compile(
    optimizer=keras.optimizers.Adam(0.001),
    loss={
        'classification': 'binary_crossentropy',
        'regression': keras.losses.Huber(delta=1.35)
    },
    loss_weights={
        'classification': 0.1,  # ë¶„ë¥˜ ê°€ì¤‘ì¹˜ ë‚®ì¶¤
        'regression': 1.0       # íšŒê·€ ì¤‘ì‹œ
    },
    metrics={
        'classification': 'accuracy',
        'regression': 'mae'
    }
)

print("   âœ… ì»´íŒŒì¼ ì™„ë£Œ")

# ============================================================
# [4] í•™ìŠµ
# ============================================================

print("\n[4] í•™ìŠµ ì‹œì‘ (ì•½ 2~3ë¶„ ì†Œìš”)")

history = model.fit(
    [X_train_numeric, X_train_gu, X_train_dong],
    [y_train_class, y_train_array],
    validation_split=0.2,
    epochs=100,
    batch_size=64,
    callbacks=[
        keras.callbacks.EarlyStopping(
            monitor='val_regression_mae',
            patience=15,
            restore_best_weights=True,
            mode='min'
        ),
        keras.callbacks.ReduceLROnPlateau(
            monitor='val_regression_mae',
            factor=0.5,
            patience=5,
            min_lr=1e-6,
            mode='min'
        )
    ],
    verbose=1
)

print(f"\n   âœ… í•™ìŠµ ì™„ë£Œ ({len(history.history['loss'])} epochs)")

# ============================================================
# [5] í‰ê°€
# ============================================================

print("\n" + "=" * 80)
print("[5] í‰ê°€")
print("=" * 80)

# ì˜ˆì¸¡
class_pred, reg_pred = model.predict(
    [X_test_numeric, X_test_gu, X_test_dong],
    verbose=0
)

class_pred = class_pred.flatten()
reg_pred = reg_pred.flatten()

# ì „ì²´ ì„±ëŠ¥
mae_total = mean_absolute_error(y_test_array, reg_pred)
print(f"\n   ì „ì²´ MAE: {mae_total:.4f}")

baseline_mae = 0.0715
improvement = (baseline_mae - mae_total) / baseline_mae * 100
print(f"   Baseline: {baseline_mae:.4f}")
print(f"   ê°œì„ : {improvement:+.1f}%")

# ì €ê°€ êµ¬ê°„
low_mask = (y_test_array < 0.5)
mae_low = mean_absolute_error(y_test_array[low_mask], reg_pred[low_mask])
baseline_low = 0.0805

print(f"\n   ì €ê°€ MAE: {mae_low:.4f}")
print(f"   Baseline: {baseline_low:.4f}")
print(f"   ê°œì„ : {(baseline_low - mae_low) / baseline_low * 100:+.1f}%")

# Within 5%p
abs_errors = np.abs(reg_pred - y_test_array)
within_5p_total = (abs_errors <= 0.05).sum() / len(abs_errors) * 100
within_5p_low = (abs_errors[low_mask] <= 0.05).sum() / low_mask.sum() * 100

print(f"\n   ì „ì²´ Within 5%p: {within_5p_total:.1f}%")
print(f"   Baseline: 49.7%")
print(f"   ê°œì„ : {within_5p_total - 49.7:+.1f}%p")

print(f"\n   ì €ê°€ Within 5%p: {within_5p_low:.1f}%")
print(f"   Baseline: 25.7%")
print(f"   ê°œì„ : {within_5p_low - 25.7:+.1f}%p")

# ë¶„ë¥˜ ì„±ëŠ¥
class_pred_binary = (class_pred > 0.5).astype(int)
acc = accuracy_score(y_test_class, class_pred_binary)
f1 = f1_score(y_test_class, class_pred_binary)

print(f"\n   ìœ ì°° ë¶„ë¥˜ ì •í™•ë„: {acc:.3f}")
print(f"   F1 Score: {f1:.3f}")

# ============================================================
# [6] í˜¼ë™í–‰ë ¬
# ============================================================

print("\n" + "=" * 80)
print("[6] í˜¼ë™í–‰ë ¬")
print("=" * 80)

def categorize_error(error):
    if error <= 0.03:
        return 'Excellent'
    elif error <= 0.05:
        return 'Good'
    elif error <= 0.10:
        return 'Fair'
    else:
        return 'Poor'

dl_categories = [categorize_error(e) for e in abs_errors]
dl_counts = Counter(dl_categories)

total = len(abs_errors)

print("\n[Entity Embedding + Multi-Task]")
print(f"   Excellent (â‰¤3%p): {dl_counts['Excellent']/total*100:.1f}%")
print(f"   Good (3~5%p):     {dl_counts['Good']/total*100:.1f}%")
print(f"   Fair (5~10%p):    {dl_counts['Fair']/total*100:.1f}%")
print(f"   Poor (>10%p):     {dl_counts['Poor']/total*100:.1f}%")

print(f"\n   Within 10%p: {((dl_counts['Excellent'] + dl_counts['Good'] + dl_counts['Fair'])/total*100):.1f}%")

# ============================================================
# [7] ìµœì¢… ìš”ì•½
# ============================================================

print("\n" + "=" * 80)
print("ğŸ† ìµœì¢… ìš”ì•½")
print("=" * 80)

print(f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                ì„±ëŠ¥ ë¹„êµ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

[ì „ì²´ ì„±ëŠ¥]
Baseline MAE:       {baseline_mae:.4f}
DL MAE:             {mae_total:.4f}
ê°œì„ :               {improvement:+.1f}%

[ì €ê°€ ì„±ëŠ¥]
Baseline MAE:       {baseline_low:.4f}
DL MAE:             {mae_low:.4f}
ê°œì„ :               {(baseline_low - mae_low) / baseline_low * 100:+.1f}%

[Within 5%p]
Baseline (ì „ì²´):    49.7%
DL (ì „ì²´):          {within_5p_total:.1f}% ({within_5p_total - 49.7:+.1f}%p)

Baseline (ì €ê°€):    25.7%
DL (ì €ê°€):          {within_5p_low:.1f}% ({within_5p_low - 25.7:+.1f}%p)

[í˜¼ë™í–‰ë ¬]
Excellent + Good:   {(dl_counts['Excellent'] + dl_counts['Good'])/total*100:.1f}%
Fair:               {dl_counts['Fair']/total*100:.1f}%
Poor:               {dl_counts['Poor']/total*100:.1f}%

[ë¶„ë¥˜ ì„±ëŠ¥]
ìœ ì°° ì˜ˆì¸¡ ì •í™•ë„:   {acc:.1%}
F1 Score:           {f1:.3f}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
""")

# í‰ê°€
if improvement > 5:
    print("ğŸ‰ğŸ‰ğŸ‰ ëŒ€í­ ê°œì„ ! Entity Embedding + Multi-Task íš¨ê³¼ í™•ì¸!")
elif improvement > 2:
    print("âœ…âœ… ê°œì„ ë¨! ë”¥ëŸ¬ë‹ íš¨ê³¼ ìˆìŒ!")
elif improvement > 0:
    print("âœ… ì†Œí­ ê°œì„ ")
else:
    print("âš ï¸ ê°œì„  ì•ˆ ë¨. ì¶”ê°€ íŠœë‹ í•„ìš”")

if within_5p_low > 35:
    print(f"ğŸ‰ğŸ‰ğŸ‰ ì €ê°€ êµ¬ê°„ ëŒ€í­ ê°œì„ ! (+{within_5p_low - 25.7:.1f}%p)")
elif within_5p_low > 30:
    print(f"âœ…âœ… ì €ê°€ êµ¬ê°„ ê°œì„ ! (+{within_5p_low - 25.7:.1f}%p)")

if dl_counts['Poor']/total*100 < 12:
    print("ğŸ‰ğŸ‰ğŸ‰ Poor êµ¬ê°„ í¬ê²Œ ê°ì†Œ! ëª©í‘œ ë‹¬ì„±!")
elif dl_counts['Poor']/total*100 < 14:
    print("âœ…âœ… Poor êµ¬ê°„ ê°ì†Œ!")

print("\n" + "=" * 80)

# ============================================================
# [8] ê²°ê³¼ ì €ì¥
# ============================================================

print("\n[8] ê²°ê³¼ ì €ì¥")

# ì˜ˆì¸¡ê°’ ì €ì¥
dl_predictions_fixed = {
    'y_test': y_test_array,
    'reg_pred': reg_pred,
    'class_pred': class_pred,
    'mae_total': mae_total,
    'mae_low': mae_low,
    'within_5p_total': within_5p_total,
    'within_5p_low': within_5p_low,
    'dl_counts': dl_counts
}

# Checkpointì— ì¶”ê°€
checkpoint['dl_predictions_fixed'] = dl_predictions_fixed
checkpoint['dl_history_fixed'] = history.history
checkpoint['le_gu'] = le_gu
checkpoint['le_dong'] = le_dong

# ì €ì¥
with open(f'{backup_dir}/checkpoint.pkl', 'wb') as f:
    pickle.dump(checkpoint, f)

print(f"   âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: checkpoint.pklì— ì¶”ê°€")
print("\n" + "=" * 80)